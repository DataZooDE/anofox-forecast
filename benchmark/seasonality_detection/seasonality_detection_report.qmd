---
title: "Seasonality Detection Methods: A Comparative Study"
subtitle: "Binary Classification Benchmark for the anofox-forecast DuckDB Extension"
author: "anofox-forecast benchmark suite"
date: today
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
    theme: cosmo
  pdf:
    toc: true
    toc-depth: 3
    colorlinks: true
    keep-tex: false
    include-in-header:
      - text: |
          \usepackage{float}
          \floatplacement{figure}{H}
execute:
  echo: false
  warning: false
  message: false
knitr:
  opts_chunk:
    fig.width: 10
    fig.height: 6
    out.width: "100%"
    fig.align: "center"
    fig.pos: "H"
---

## Executive Summary

This benchmark evaluates seasonality detection methods as a **binary classification problem**: given a time series, does it contain seasonality? We simulate 550 curves with varying seasonal strength levels (0.0 to 1.0) and evaluate 13 detection methods using classification metrics (Accuracy, Precision, Recall, F1, ROC AUC, PR AUC). Ground truth is defined as seasonal if simulated strength >= 0.2.

**Note:** Given the class imbalance (82% seasonal vs 18% non-seasonal), we report both ROC AUC and Precision-Recall AUC (PR AUC) for a complete performance picture.

This benchmark replicates the methodology from the fdars R package benchmark.

### Quick Start: Which Method Should I Use?

| Your Situation | Recommended Method | SQL Example |
|----------------|-------------------|-------------|
| **General purpose** | Wavelet or Variance Strength | `ts_seasonal_strength(values, period, 'wavelet')` |
| **Unknown period** | Autoperiod | `(ts_autoperiod(values)).detected` |
| **Fast screening** | FFT | `(ts_estimate_period_fft(values)).confidence` |
| **Trending data** | CFD-Autoperiod | `(ts_cfd_autoperiod(values)).detected` |
| **Noisy data** | ACF or Autoperiod | `(ts_estimate_period_acf(values)).confidence` |
| **Irregular sampling** | Lomb-Scargle | `(ts_lomb_scargle(values)).false_alarm_prob` |
| **Need model fit** | AIC | `(ts_aic_period(values)).r_squared` |

### Quick SQL Example

```sql
-- From long format data (one row per observation):
-- Aggregate values per series, then detect seasonality
SELECT
    series_id,
    (ts_autoperiod(LIST(value ORDER BY date))).detected AS has_seasonality,
    (ts_autoperiod(LIST(value ORDER BY date))).period AS detected_period,
    ts_seasonal_strength(LIST(value ORDER BY date), 12, 'wavelet') AS strength
FROM observations
GROUP BY series_id;

-- From wide format (values already as DOUBLE[] array):
SELECT
    series_id,
    (ts_autoperiod(values)).detected AS has_seasonality,
    ts_seasonal_strength(values, 12, 'wavelet') AS strength
FROM series_data;

-- Threshold: strength > 0.3 typically indicates seasonality
```

### Key Results (TL;DR)

- **Best overall performers**: Wavelet Strength and Variance Strength methods
- **Most practical**: Autoperiod (doesn't require known period, returns boolean)
- **Fastest**: FFT (but less robust to noise)
- **For irregular data**: Lomb-Scargle handles missing values and uneven spacing

*For detailed methodology and analysis, continue reading below.*

```{r}
#| label: setup
#| cache: false

library(DBI)
library(duckdb)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(knitr)
library(scales)
library(pROC)
library(PRROC)

# Source helper files (colorblind-friendly palette and plotting functions)
source("R/colors.R")
source("R/plots.R")

# Set ggplot theme
theme_set(theme_minimal(base_size = 12))
```

## Introduction

### Detection as Binary Classification

Unlike period estimation (which asks "what is the period?"), **seasonality detection** asks a simpler question: **"is there seasonality?"** This is a binary classification problem where each method produces a confidence score, and we apply a threshold to make a detection decision.

### Hypotheses

Based on method characteristics, we predict:

1. **Spectral methods** (FFT, Lomb-Scargle) will excel on clean, stationary data but degrade with autocorrelated noise (red noise)
2. **Differencing-based methods** (CFD-Autoperiod) will outperform others on trending data by removing non-stationarity
3. **Variance-based methods** will show the best overall robustness due to their non-parametric nature
4. **Pattern-matching methods** (Matrix Profile, ACF) will struggle with short series where insufficient cycles are present

### Methods Evaluated

| Method | SQL Function | Score Used | Description |
|--------|-------------|------------|-------------|
| AIC Comparison | `ts_aic_period` | R-squared | Fourier model fit quality |
| FFT Confidence | `ts_estimate_period_fft` | confidence | Peak-to-mean power ratio |
| ACF Confidence | `ts_estimate_period_acf` | confidence | Autocorrelation at lag |
| Variance Strength | `ts_seasonal_strength(..., 'variance')` | strength | Seasonal variance ratio |
| Spectral Strength | `ts_seasonal_strength(..., 'spectral')` | strength | Power at seasonal frequency |
| Wavelet Strength | `ts_seasonal_strength(..., 'wavelet')` | strength | Morlet wavelet energy |
| SAZED | `ts_sazed_period` | SNR | Zero-padded spectral SNR |
| Autoperiod | `ts_autoperiod` | acf_validation | FFT+ACF hybrid validation |
| CFD-Autoperiod | `ts_cfd_autoperiod` | acf_validation | First-differenced FFT+ACF |
| Lomb-Scargle | `ts_lomb_scargle` | 1-FAP | Statistical significance |
| Matrix Profile | `ts_matrix_profile_period` | confidence | Motif agreement ratio |
| STL | `ts_stl_period` | seasonal_strength | Decomposition strength |
| SSA | `ts_ssa_period` | variance_explained | Eigenvalue dominance |

### Detailed Method Descriptions

#### Spectral Methods

**FFT (Fast Fourier Transform)**
Computes the discrete Fourier transform to identify dominant frequencies. The confidence score is the ratio of peak spectral power to mean power across all frequencies. Fast ($O(n \log n)$) but sensitive to noise and non-stationarity.

$$X[k] = \sum_{t=0}^{N-1} x[t] \cdot e^{-2\pi ikt/N}, \quad \text{Confidence} = \frac{P[k_{max}]}{\bar{P}}$$

*Reference: Cooley, J.W. & Tukey, J.W. (1965). "An Algorithm for the Machine Calculation of Complex Fourier Series." Mathematics of Computation, 19(90), 297-301.*

**Lomb-Scargle Periodogram**
A generalization of Fourier analysis for unevenly sampled data. Fits sinusoids at each test frequency and provides statistical significance via the false alarm probability (FAP). Robust for irregular sampling.

$$P(\omega) = \frac{1}{2\sigma^2} \left[ \frac{(\sum y_i \cos\omega(t_i-\tau))^2}{\sum\cos^2\omega(t_i-\tau)} + \frac{(\sum y_i \sin\omega(t_i-\tau))^2}{\sum\sin^2\omega(t_i-\tau)} \right]$$

*References: Lomb, N.R. (1976). "Least-squares frequency analysis of unequally spaced data." Astrophysics and Space Science, 39, 447-462. Scargle, J.D. (1982). "Studies in astronomical time series analysis II." The Astrophysical Journal, 263, 835-853.*

**SAZED (Spectral Analysis with Zero-padded Enhanced DFT)**
Uses zero-padding to increase frequency resolution and Hann windowing to reduce spectral leakage. The signal-to-noise ratio (SNR) provides a confidence measure.

*Reference: Ding, H., et al. (2008). "Querying and Mining of Time Series Data." VLDB Endowment, 1(2), 1542-1552.*

#### Autocorrelation Methods

**ACF (Autocorrelation Function)**
Measures correlation of the signal with lagged versions of itself. Peaks in the ACF indicate periodic structure. The confidence is the ACF value at the detected period lag.

$$\text{ACF}(k) = \frac{\sum_{t=1}^{n-k} (x_t - \mu)(x_{t+k} - \mu)}{\sum_{t=1}^{n} (x_t - \mu)^2}$$

*Reference: Box, G.E.P. & Jenkins, G.M. (1976). Time Series Analysis: Forecasting and Control. Holden-Day.*

**Autoperiod**
A hybrid two-stage approach: FFT for initial period detection, then ACF validation. Combines spectral speed with time-domain robustness.

*Reference: Vlachos, M., Yu, P., & Castelli, V. (2005). "On Periodicity Detection and Structural Periodic Similarity." SIAM International Conference on Data Mining.*

**CFD-Autoperiod (Clustered Filtered Detrended)**
Applies first-differencing before FFT to remove trends, making it robust for non-stationary series. Validates with ACF on the original series.

*Reference: Elfeky, M.G., Aref, W.G., & Elmagarmid, A.K. (2005). "Periodicity Detection in Time Series Databases." IEEE TKDE, 17(7), 875-887.*

#### Model-Based Methods

**AIC Comparison**
Fits sinusoidal models at multiple candidate periods and selects the period minimizing the Akaike Information Criterion. Returns R² as a measure of model fit quality.

$$\text{AIC} = n \cdot \ln(\text{RSS}/n) + 2k, \quad R^2 = 1 - \frac{\text{RSS}}{\text{SS}_{total}}$$

*Reference: Akaike, H. (1974). "A new look at the statistical model identification." IEEE Transactions on Automatic Control, 19(6), 716-723.*

#### Decomposition Methods

**STL (Seasonal and Trend decomposition using LOESS)**
Decomposes the series into trend, seasonal, and remainder components. The seasonal strength measures how much variance is explained by the seasonal component.

$$F_S = \max\left(0, 1 - \frac{\text{Var}(R)}{\text{Var}(S + R)}\right)$$

*Reference: Cleveland, R.B., et al. (1990). "STL: A Seasonal-Trend Decomposition Procedure Based on Loess." Journal of Official Statistics, 6(1), 3-73.*

**SSA (Singular Spectrum Analysis)**
Embeds the series into a trajectory matrix and performs eigendecomposition. Periodic components appear as paired eigenvalues. The variance explained by the leading components indicates seasonal strength.

*Reference: Golyandina, N., Nekrutkin, V., & Zhigljavsky, A. (2001). Analysis of Time Series Structure: SSA and Related Techniques. Chapman & Hall/CRC.*

#### Strength-Based Methods

**Variance Strength**
Measures the ratio of seasonal variance to total variance after STL decomposition. Values near 1 indicate strong seasonality.

**Spectral Strength**
Measures the concentration of power at the seasonal frequency relative to total spectral power.

**Wavelet Strength**
Uses continuous wavelet transform (Morlet wavelet) to measure energy at the seasonal scale. Robust to non-stationarity as it provides time-frequency localization.

*Reference: Wang, X., Smith, K., & Hyndman, R. (2006). "Characteristic-based clustering for time series data." Data Mining and Knowledge Discovery, 13(3), 335-364.*

#### Pattern-Based Methods

**Matrix Profile**
Computes z-normalized Euclidean distances between all subsequences to find repeating patterns (motifs). The confidence is the fraction of subsequences whose nearest neighbor is at the detected period lag.

$$d(i,j) = \sqrt{\sum(z_i - z_j)^2}, \quad \text{Period} = \arg\max_k H[k]$$

*References: Yeh, C.C.M., et al. (2016). "Matrix Profile I: All Pairs Similarity Joins for Time Series." IEEE ICDM. Yeh, C.C.M., et al. (2017). "Matrix Profile VI: Meaningful Multidimensional Motif Discovery." IEEE ICDM.*

### Ground Truth Definition

A series is classified as **seasonal** if its simulated seasonal strength >= 0.2. This threshold follows the fdars benchmark convention.

## Setup

### Connect to DuckDB and Load Extension

```{r}
#| label: connect-duckdb
#| cache: false

con <- dbConnect(duckdb(config = list("allow_unsigned_extensions" = "true")))

extension_path <- "../../build/release/extension/anofox_forecast/anofox_forecast.duckdb_extension"

if (file.exists(extension_path)) {
  dbExecute(con, sprintf("LOAD '%s'", extension_path))
  message("Extension loaded successfully!")
} else {
  warning("Extension not found at: ", extension_path,
          "\nPlease build the extension first with 'make' in the project root.")
}
```

## Baseline Simulation

### Simulation Parameters

Following the fdars benchmark:
- **11 strength levels**: 0.0, 0.1, 0.2, ..., 1.0
- **50 curves per level**: 550 total curves
- **60 observations**: 5 years of monthly data
- **Period = 12**: Monthly seasonality
- **White noise**: sigma = 0.3

```{r}
#| label: simulation-params

N_LEVELS <- 11
N_CURVES_PER_LEVEL <- 50
N_TOTAL <- N_LEVELS * N_CURVES_PER_LEVEL  # 550
N_POINTS <- 60
PERIOD <- 12.0
NOISE_SD <- 0.3
STRENGTH_THRESHOLD <- 0.2
SEED <- 42

set.seed(SEED)
```

### Baseline Data Generation

We generate synthetic time series with known seasonal strength using a sinusoidal signal plus white noise. The amplitude is calibrated so that the signal-to-noise ratio corresponds to the target strength level: $\text{strength} = A^2 / (A^2 + \sigma^2)$.

```{r}
#| label: generate-baseline

# Generate baseline curves with varying seasonal strength
generate_baseline <- function(n_levels, n_per_level, n_points, period, noise_sd) {
  strength_levels <- seq(0, 1, length.out = n_levels)

  curves <- map_dfr(seq_along(strength_levels), function(level_idx) {
    strength <- strength_levels[level_idx]

    map_dfr(1:n_per_level, function(curve_idx) {
      t <- 0:(n_points - 1)
      phase <- runif(1, 0, 2 * pi)

      # Amplitude derived from strength: strength = amp^2 / (amp^2 + noise^2)
      # So amp = sqrt(strength * noise^2 / (1 - strength)) when strength < 1
      if (strength > 0 && strength < 1) {
        amplitude <- sqrt(strength * noise_sd^2 / (1 - strength))
      } else if (strength >= 1) {
        amplitude <- 10 * noise_sd  # Very strong signal
      } else {
        amplitude <- 0
      }

      seasonal <- amplitude * sin(2 * pi * t / period + phase)
      noise <- rnorm(n_points, 0, noise_sd)
      values <- seasonal + noise

      tibble(
        curve_id = (level_idx - 1) * n_per_level + curve_idx,
        strength_level = strength,
        is_seasonal = strength >= STRENGTH_THRESHOLD,
        scenario = "baseline",
        values = list(values)
      )
    })
  })

  curves
}

baseline_data <- generate_baseline(N_LEVELS, N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD)

cat(sprintf("Generated %d curves\n", nrow(baseline_data)))
cat(sprintf("Seasonal (strength >= %.1f): %d\n", STRENGTH_THRESHOLD, sum(baseline_data$is_seasonal)))
cat(sprintf("Non-seasonal: %d\n", sum(!baseline_data$is_seasonal)))
```

### Strength Level Distribution

```{r}
#| label: fig-strength-dist
#| fig-cap: "Distribution of curves by seasonal strength level"

ggplot(baseline_data, aes(x = factor(strength_level), fill = is_seasonal)) +
  geom_bar() +
  scale_fill_manual(values = c("TRUE" = "#1a9850", "FALSE" = "#d73027"),
                    labels = c("TRUE" = "Seasonal", "FALSE" = "Non-seasonal")) +
  labs(
    title = "Curve Distribution by Strength Level",
    subtitle = sprintf("Ground truth: seasonal if strength >= %.1f", STRENGTH_THRESHOLD),
    x = "Seasonal Strength",
    y = "Number of Curves",
    fill = "Ground Truth"
  )
```

### Example Curves

```{r}
#| label: fig-example-curves
#| fig-cap: "Example curves at different strength levels"
#| fig-height: 7

examples <- baseline_data %>%
  filter(strength_level %in% c(0.0, 0.2, 0.5, 0.8, 1.0)) %>%
  group_by(strength_level) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(
    time = map(values, ~ 1:length(.x))
  ) %>%
  unnest(c(time, values))

ggplot(examples, aes(x = time, y = values)) +
  geom_line(color = "steelblue", linewidth = 0.5) +
  facet_wrap(~sprintf("Strength = %.1f", strength_level), ncol = 1, scales = "free_y") +
  labs(title = "Example Curves at Different Strength Levels", x = "Time", y = "Value")
```

### Load Data into DuckDB

The simulated curves are loaded into a DuckDB table for analysis. Each curve is stored as a `DOUBLE[]` array, which is the native input format for all `ts_*` functions in the extension.

```{r}
#| label: load-to-duckdb
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "
  CREATE TABLE benchmark_curves (
    curve_id INTEGER,
    strength_level DOUBLE,
    is_seasonal BOOLEAN,
    scenario VARCHAR,
    values DOUBLE[]
  )
")

for (i in 1:nrow(baseline_data)) {
  row <- baseline_data[i, ]
  values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")

  dbExecute(con, sprintf("
    INSERT INTO benchmark_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
  ", row$curve_id, row$strength_level,
     ifelse(row$is_seasonal, "true", "false"),
     row$scenario, values_str))
}

cat("Data loaded into DuckDB\n")
```

## Method Evaluation

### SQL API Usage

The following examples demonstrate how to use the seasonality detection methods. All `ts_*` functions expect a `DOUBLE[]` array as input.

**Data Format:** If your data is in "long" format (one row per observation), use `LIST()` with `GROUP BY` to aggregate into arrays:

```sql
-- Long format: one row per observation
-- +----------+------------+-------+
-- | series_id| date       | value |
-- +----------+------------+-------+
-- | A        | 2020-01-01 | 10.5  |
-- | A        | 2020-02-01 | 12.3  |
-- | B        | 2020-01-01 | 5.2   |
-- +----------+------------+-------+

-- Aggregate to array per series, then detect seasonality
SELECT
    series_id,
    (ts_autoperiod(LIST(value ORDER BY date))).detected AS has_seasonality,
    (ts_autoperiod(LIST(value ORDER BY date))).period AS detected_period
FROM long_format_data
GROUP BY series_id;
```

**Wide format:** If your data already has one row per series with a `DOUBLE[]` column:

```sql
-- Wide format: one row per series with array column
-- +----------+---------------------------+
-- | series_id| values                    |
-- +----------+---------------------------+
-- | A        | [10.5, 12.3, 11.8, ...]   |
-- | B        | [5.2, 6.1, 4.8, ...]      |
-- +----------+---------------------------+

SELECT
    series_id,

    -- Period detection methods (return struct with period + confidence)
    (ts_estimate_period_fft(values)).period AS fft_period,
    (ts_estimate_period_fft(values)).confidence AS fft_confidence,

    (ts_estimate_period_acf(values)).period AS acf_period,
    (ts_estimate_period_acf(values)).confidence AS acf_confidence,

    -- Autoperiod methods (FFT + ACF validation)
    (ts_autoperiod(values)).period AS autoperiod_period,
    (ts_autoperiod(values)).detected AS autoperiod_detected,
    (ts_autoperiod(values)).acf_validation AS autoperiod_score,

    (ts_cfd_autoperiod(values)).period AS cfd_period,
    (ts_cfd_autoperiod(values)).acf_validation AS cfd_score,

    -- Model-based methods
    (ts_aic_period(values)).period AS aic_period,
    (ts_aic_period(values)).r_squared AS aic_r_squared,

    -- Spectral methods
    (ts_lomb_scargle(values)).period AS lomb_period,
    (ts_lomb_scargle(values)).false_alarm_prob AS lomb_fap,

    (ts_sazed_period(values)).period AS sazed_period,
    (ts_sazed_period(values)).snr AS sazed_snr,

    -- Decomposition methods
    (ts_stl_period(values)).period AS stl_period,
    (ts_stl_period(values)).seasonal_strength AS stl_strength,

    (ts_ssa_period(values)).period AS ssa_period,
    (ts_ssa_period(values)).variance_explained AS ssa_variance,

    -- Pattern-based methods
    (ts_matrix_profile_period(values)).period AS mp_period,
    (ts_matrix_profile_period(values)).confidence AS mp_confidence,

    -- Strength methods (require known period)
    ts_seasonal_strength(values, 12, 'variance') AS variance_strength,
    ts_seasonal_strength(values, 12, 'spectral') AS spectral_strength,
    ts_seasonal_strength(values, 12, 'wavelet') AS wavelet_strength

FROM wide_format_data;
```

### Extract Confidence Scores

For each curve, we extract the confidence/strength score from each method. Scores are normalized to [0, 1] where possible for fair comparison.

```{r}
#| label: extract-scores
#| cache: false

# Run all detection methods and extract confidence scores
# Use known period = 12 for strength methods
scores <- dbGetQuery(con, "
SELECT
    curve_id,
    strength_level,
    is_seasonal,
    scenario,
    -- AIC: R-squared as confidence
    (ts_aic_period(values)).r_squared as aic_score,
    -- FFT: confidence (peak/mean power ratio), normalize to 0-1
    LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
    -- ACF: confidence (correlation value)
    (ts_estimate_period_acf(values)).confidence as acf_score,
    -- Strength methods (with known period = 12)
    ts_seasonal_strength(values, 12, 'variance') as variance_score,
    ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
    ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
    -- SAZED: SNR normalized
    LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
    -- Autoperiod: ACF validation
    (ts_autoperiod(values)).acf_validation as autoperiod_score,
    -- CFD-Autoperiod: ACF validation
    (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
    -- Lomb-Scargle: 1 - false alarm probability
    1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
    -- Matrix Profile: confidence
    (ts_matrix_profile_period(values)).confidence as mp_score,
    -- STL: seasonal strength
    (ts_stl_period(values)).seasonal_strength as stl_score,
    -- SSA: variance explained
    (ts_ssa_period(values)).variance_explained as ssa_score
FROM benchmark_curves
")

# Convert to tibble and handle NaN/Inf
scores <- as_tibble(scores) %>%
  mutate(across(ends_with("_score"), ~ ifelse(is.nan(.) | is.infinite(.), 0, .)))

cat(sprintf("Extracted scores for %d curves\n", nrow(scores)))
```

### Score Distributions by Ground Truth

```{r}
#| label: fig-score-distributions
#| fig-cap: "Distribution of confidence scores by ground truth (seasonal vs non-seasonal)"
#| fig-height: 9

score_long <- scores %>%
  pivot_longer(
    cols = ends_with("_score"),
    names_to = "method",
    values_to = "score"
  ) %>%
  mutate(
    method = gsub("_score", "", method),
    method = case_when(
      method == "aic" ~ "AIC",
      method == "fft" ~ "FFT",
      method == "acf" ~ "ACF",
      method == "variance" ~ "Variance",
      method == "spectral" ~ "Spectral",
      method == "wavelet" ~ "Wavelet",
      method == "sazed" ~ "SAZED",
      method == "autoperiod" ~ "Autoperiod",
      method == "cfd" ~ "CFD",
      method == "lomb" ~ "Lomb",
      method == "mp" ~ "MatrixProfile",
      method == "stl" ~ "STL",
      method == "ssa" ~ "SSA"
    ),
    method = factor(method, levels = names(METHOD_COLORS)),
    ground_truth = ifelse(is_seasonal, "Seasonal", "Non-seasonal")
  )

ggplot(score_long, aes(x = score, fill = ground_truth)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~method, scales = "free", ncol = 3) +
  scale_fill_manual(values = c("Seasonal" = "#1a9850", "Non-seasonal" = "#d73027")) +
  labs(
    title = "Confidence Score Distributions by Ground Truth",
    subtitle = "Good separation indicates discriminative power",
    x = "Confidence Score",
    y = "Density",
    fill = "Ground Truth"
  ) +
  theme(legend.position = "bottom")
```

## ROC Analysis

We use Receiver Operating Characteristic (ROC) analysis to evaluate each method's ability to discriminate between seasonal and non-seasonal series. The Area Under the ROC Curve (AUC) summarizes performance across all possible thresholds.

Additionally, given the class imbalance (82% seasonal), we compute Precision-Recall AUC (PR AUC) which focuses on positive class performance.

```{r}
#| label: compute-roc

methods <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
             "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

method_labels <- c(
  "aic" = "AIC", "fft" = "FFT", "acf" = "ACF",
  "variance" = "Variance", "spectral" = "Spectral", "wavelet" = "Wavelet",
  "sazed" = "SAZED", "autoperiod" = "Autoperiod", "cfd" = "CFD",
  "lomb" = "Lomb", "mp" = "MatrixProfile", "stl" = "STL", "ssa" = "SSA"
)

# Compute ROC and PR curves for each method
roc_results <- map(methods, function(m) {
  score_col <- paste0(m, "_score")
  score_vals <- scores[[score_col]]

  # Handle NA values
  valid_idx <- !is.na(score_vals)
  if (sum(valid_idx) < 10) return(NULL)

  roc_obj <- roc(scores$is_seasonal[valid_idx], score_vals[valid_idx],
                 quiet = TRUE, direction = "<")

  # Get optimal threshold (Youden's J)
  coords_best <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))

  # Compute PR AUC using PRROC package
  # scores.class0 = scores for positive class (seasonal)
  # scores.class1 = scores for negative class (non-seasonal)
  pr_obj <- pr.curve(
    scores.class0 = score_vals[valid_idx & scores$is_seasonal[valid_idx]],
    scores.class1 = score_vals[valid_idx & !scores$is_seasonal[valid_idx]],
    curve = FALSE
  )

  list(
    method = method_labels[m],
    roc = roc_obj,
    auc = auc(roc_obj),
    pr_auc = pr_obj$auc.integral,
    threshold = coords_best$threshold,
    sensitivity = coords_best$sensitivity,
    specificity = coords_best$specificity
  )
}) %>%
  compact()

# Create AUC summary table
auc_summary <- map_dfr(roc_results, function(r) {
  tibble(
    Method = r$method,
    `ROC AUC` = as.numeric(r$auc),
    `PR AUC` = r$pr_auc,
    `Optimal Threshold` = r$threshold,
    Sensitivity = r$sensitivity,
    Specificity = r$specificity
  )
}) %>%
  arrange(desc(`ROC AUC`))

kable(auc_summary, digits = 3, caption = "ROC and PR Analysis Summary (sorted by ROC AUC)")
```

### ROC Curves

```{r}
#| label: fig-roc-curves
#| fig-cap: "ROC Curves by Method Family: Spectral methods (FFT, Lomb) show similar curves, while decomposition methods (AIC, STL, SSA) have more varied performance. Pattern-based methods generally achieve higher AUC."
#| fig-height: 8

# Create named list of ROC objects for the faceted plot function
roc_objects <- setNames(
  lapply(roc_results, function(r) r$roc),
  sapply(roc_results, function(r) r$method)
)

# Use the faceted plotting function
plot_roc_curves_faceted(roc_objects, METHOD_COLORS, METHOD_TO_FAMILY)
```

### AUC Comparison

```{r}
#| label: fig-auc-comparison
#| fig-cap: "ROC AUC and PR AUC comparison across methods"
#| fig-height: 7

# Reshape for faceted plot
auc_long <- auc_summary %>%
  select(Method, `ROC AUC`, `PR AUC`) %>%
  pivot_longer(cols = c(`ROC AUC`, `PR AUC`), names_to = "Metric", values_to = "AUC") %>%
  mutate(Metric = factor(Metric, levels = c("ROC AUC", "PR AUC")))

ggplot(auc_long, aes(x = reorder(Method, AUC), y = AUC, fill = Method)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.3f", AUC)), hjust = -0.1, size = 2.5) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1.15)) +
  coord_flip() +
  facet_wrap(~Metric, ncol = 2) +
  labs(
    title = "Area Under Curve Comparison",
    subtitle = "ROC AUC: overall discrimination | PR AUC: performance on positive class (82% imbalance)",
    x = "",
    y = "AUC"
  ) +
  theme(legend.position = "none")
```

### Optimal Threshold Estimation

The **optimal classification threshold** for each method is determined using **Youden's J statistic** (Youden, 1950), which maximizes the sum of sensitivity and specificity:

$$J = \text{Sensitivity} + \text{Specificity} - 1 = \text{TPR} - \text{FPR}$$

The threshold that maximizes $J$ represents the point on the ROC curve farthest from the diagonal (random classifier), providing the best trade-off between detecting true seasonality (sensitivity) and avoiding false positives (specificity).

**Why Youden's J?**

- **Balanced approach**: Does not favor sensitivity over specificity
- **Threshold-independent**: Works for any score distribution
- **Standard practice**: Widely used in diagnostic test evaluation

Alternative threshold selection methods include:

- **Fixed sensitivity**: Set threshold to achieve target sensitivity (e.g., 0.90)
- **Cost-based**: Minimize misclassification cost when FP and FN have different costs
- **Prevalence-adjusted**: Account for class imbalance in threshold selection

The `pROC::coords()` function with `best` method implements Youden's J optimization.

**Reference**: Youden, W. J. (1950). Index for rating diagnostic tests. *Cancer*, 3(1), 32-35.

## Runtime Performance

Computational efficiency matters for production deployments. We benchmark each method's execution time per 1000 series using our synthetic dataset.

```{r}
#| label: runtime-benchmark
#| fig-cap: "Execution Time per 1000 Series: Variance-based methods are fastest, while decomposition methods (STL, SSA) have higher computational cost. All methods complete in under 500ms for 1000 series, making them suitable for batch processing."
#| fig-height: 5

# Benchmark each method using benchmark_curves table (already loaded)
benchmark_runtime <- function(con, method_name, sql_expr) {
  # Use LIMIT 1000 for consistent timing
  timing <- system.time({
    dbExecute(con, sprintf("
      SELECT curve_id, %s as result
      FROM benchmark_curves
      LIMIT 1000
    ", sql_expr))
  })
  tibble(Method = method_name, Runtime_ms = timing["elapsed"] * 1000)
}

# Benchmark each method (using correct function signatures)
runtime_results <- bind_rows(
  benchmark_runtime(con, "Variance",
    "ts_seasonal_strength(values, 12, 'variance')"),
  benchmark_runtime(con, "FFT",
    "(ts_estimate_period_fft(values)).confidence"),
  benchmark_runtime(con, "ACF",
    "(ts_estimate_period_acf(values)).confidence"),
  benchmark_runtime(con, "Spectral",
    "ts_seasonal_strength(values, 12, 'spectral')"),
  benchmark_runtime(con, "Wavelet",
    "ts_seasonal_strength(values, 12, 'wavelet')"),
  benchmark_runtime(con, "SAZED",
    "(ts_sazed_period(values)).snr"),
  benchmark_runtime(con, "Autoperiod",
    "(ts_autoperiod(values)).acf_validation"),
  benchmark_runtime(con, "CFD",
    "(ts_cfd_autoperiod(values)).acf_validation"),
  benchmark_runtime(con, "Lomb",
    "1.0 - (ts_lomb_scargle(values)).false_alarm_prob"),
  benchmark_runtime(con, "MatrixProfile",
    "(ts_matrix_profile_period(values)).confidence"),
  benchmark_runtime(con, "STL",
    "(ts_stl_period(values)).seasonal_strength"),
  benchmark_runtime(con, "SSA",
    "(ts_ssa_period(values)).variance_explained"),
  benchmark_runtime(con, "AIC",
    "(ts_aic_period(values)).r_squared")
)

# Plot runtime comparison
plot_runtime_comparison(runtime_results, METHOD_COLORS)
```

## Classification Performance

Using the optimal threshold from ROC analysis (Youden's J statistic), we convert continuous scores into binary predictions and compute standard classification metrics.

```{r}
#| label: apply-thresholds

# Create predictions using optimal thresholds
predictions <- scores %>%
  select(curve_id, is_seasonal)

for (r in roc_results) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  score_col <- paste0(method_lower, "_score")
  pred_col <- paste0(method_lower, "_pred")

  if (score_col %in% names(scores)) {
    predictions[[pred_col]] <- scores[[score_col]] >= r$threshold
  }
}

# Handle any remaining NAs
predictions <- predictions %>%
  mutate(across(ends_with("_pred"), ~ replace_na(., FALSE)))
```

We calculate Accuracy, Precision (positive predictive value), Recall (sensitivity), Specificity, False Positive Rate, and F1 score for each method.

```{r}
#| label: classification-metrics

calc_metrics <- function(predicted, actual) {
  predicted <- replace(predicted, is.na(predicted), FALSE)
  actual <- replace(actual, is.na(actual), FALSE)

  tp <- sum(predicted & actual)
  tn <- sum(!predicted & !actual)
  fp <- sum(predicted & !actual)
  fn <- sum(!predicted & actual)

  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  specificity <- ifelse(tn + fp > 0, tn / (tn + fp), 0)
  fpr <- ifelse(fp + tn > 0, fp / (fp + tn), 0)
  f1 <- ifelse(precision + recall > 0, 2 * precision * recall / (precision + recall), 0)

  tibble(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    Specificity = specificity,
    FPR = fpr,
    F1 = f1
  )
}

# Calculate metrics for each method
metrics_list <- map(roc_results, function(r) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  pred_col <- paste0(method_lower, "_pred")

  if (pred_col %in% names(predictions)) {
    calc_metrics(predictions[[pred_col]], predictions$is_seasonal) %>%
      mutate(Method = r$method)
  } else {
    NULL
  }
}) %>%
  compact()

metrics <- bind_rows(metrics_list) %>%
  select(Method, Accuracy, Precision, Recall, Specificity, FPR, F1) %>%
  arrange(desc(F1))

kable(metrics, digits = 3, caption = "Classification Performance at Optimal Thresholds (sorted by F1)")
```

### Performance Comparison

```{r}
#| label: fig-performance-comparison
#| fig-cap: "Classification metrics comparison across methods"
#| fig-height: 6

metrics_long <- metrics %>%
  pivot_longer(cols = c(Accuracy, Precision, Recall, F1),
               names_to = "Metric", values_to = "Value")

ggplot(metrics_long, aes(x = Method, y = Value, fill = Method)) +
  geom_col() +
  facet_wrap(~Metric, ncol = 2) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1), labels = percent) +
  labs(
    title = "Classification Performance Metrics by Method",
    x = "",
    y = "Score"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    legend.position = "none",
    plot.margin = margin(5, 15, 5, 5)
  )
```

### Confusion Matrices

Confusion matrices show the detailed breakdown of true positives, false positives, true negatives, and false negatives for the top-performing methods.

```{r}
#| label: confusion-matrices
#| fig-cap: "Confusion Matrices for Top 4 Methods: Variance and Wavelet achieve the best balance of sensitivity and specificity. Note that false negatives (missing seasonal patterns) are generally more costly than false positives in forecasting applications."
#| fig-height: 6

# Create prediction data for confusion matrices
pred_data <- list()
top_methods <- c("Variance", "Wavelet", "FFT", "ACF")

for (method in top_methods) {
  method_lower <- tolower(method)
  if (method == "MatrixProfile") method_lower <- "mp"

  pred_col <- paste0(method_lower, "_pred")
  if (!pred_col %in% names(predictions)) next

  pred_data[[method]] <- tibble(
    predicted = ifelse(predictions[[pred_col]], "Seasonal", "Non-Seasonal"),
    actual = ifelse(predictions$is_seasonal, "Seasonal", "Non-Seasonal")
  )
}

# Plot faceted confusion matrices
plot_confusion_matrices_faceted(pred_data, top_methods)
```

## Statistical Significance: McNemar Tests

McNemar's test compares paired binary predictions between methods. A significant p-value indicates methods differ in their detection decisions.

```{r}
#| label: mcnemar-tests

run_mcnemar <- function(pred1, pred2) {
  pred1 <- replace(pred1, is.na(pred1), FALSE)
  pred2 <- replace(pred2, is.na(pred2), FALSE)

  b <- sum(pred1 & !pred2)  # Method1 positive, Method2 negative

  c <- sum(!pred1 & pred2)  # Method1 negative, Method2 positive

  if (b + c > 0) {
    chi_sq <- (abs(b - c) - 1)^2 / (b + c)
    p_value <- pchisq(chi_sq, df = 1, lower.tail = FALSE)
  } else {
    chi_sq <- 0
    p_value <- 1
  }

  list(chi_sq = chi_sq, p_value = p_value, b = b, c = c)
}

# Get all prediction columns
pred_cols <- names(predictions)[grepl("_pred$", names(predictions))]
method_names <- gsub("_pred$", "", pred_cols)

# Run pairwise McNemar tests
mcnemar_results <- expand.grid(
  method1 = method_names,
  method2 = method_names,
  stringsAsFactors = FALSE
) %>%
  filter(method1 < method2) %>%
  rowwise() %>%
  mutate(
    test = list(run_mcnemar(
      predictions[[paste0(method1, "_pred")]],
      predictions[[paste0(method2, "_pred")]]
    )),
    chi_sq = test$chi_sq,
    p_value = test$p_value
  ) %>%
  ungroup() %>%
  select(-test) %>%
  # Apply Benjamini-Hochberg (FDR) correction for multiple testing
  # With 13 methods, we have C(13,2) = 78 pairwise comparisons
  mutate(
    p_adjusted = p.adjust(p_value, method = "BH"),
    significant = p_adjusted < 0.05,
    method1 = method_labels[method1],
    method2 = method_labels[method2]
  )

cat(sprintf("Note: p-values adjusted for %d pairwise comparisons using Benjamini-Hochberg (FDR) correction.\n\n",
            nrow(mcnemar_results)))

# Show significant differences
significant_pairs <- mcnemar_results %>%
  filter(significant) %>%
  arrange(p_adjusted)

if (nrow(significant_pairs) > 0) {
  kable(significant_pairs %>% select(method1, method2, chi_sq, p_value, p_adjusted),
        digits = 4, col.names = c("Method 1", "Method 2", "Chi-sq", "p (raw)", "p (adjusted)"),
        caption = "Significant McNemar Test Results (adjusted p < 0.05)")
} else {
  cat("No significant differences found between any method pairs after FDR correction.\n")
}
```

### McNemar P-Value Heatmap (FDR-Adjusted)

```{r}
#| label: fig-mcnemar-heatmap
#| fig-cap: "McNemar test p-values (FDR-adjusted) between method pairs (red = significant difference)"
#| fig-height: 7

# Create symmetric matrix with adjusted p-values
mcnemar_matrix <- mcnemar_results %>%
  select(method1, method2, p_adjusted) %>%
  bind_rows(
    mcnemar_results %>%
      rename(method1 = method2, method2 = method1) %>%
      select(method1, method2, p_adjusted)
  )

ggplot(mcnemar_matrix, aes(x = method1, y = method2, fill = p_adjusted)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = ifelse(p_adjusted < 0.001, "<.001", sprintf("%.3f", p_adjusted))),
            size = 2.5) +
  scale_fill_gradient2(low = "#d73027", mid = "#ffffbf", high = "#1a9850",
                       midpoint = 0.5, limits = c(0, 1)) +
  labs(
    title = "McNemar Test P-Values Between Methods",
    subtitle = "FDR-adjusted (Benjamini-Hochberg); Red = significant difference (adj. p < 0.05)",
    x = "", y = "",
    fill = "Adj. p-value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
    axis.text.y = element_text(size = 7),
    plot.margin = margin(5, 15, 5, 5)
  )
```

\newpage

## Challenge Scenarios

Following the fdars benchmark, we test method robustness under challenging conditions.

### Challenge 1: Linear Trends

Linear trends are common in real-world data and can mask or mimic seasonality. We add trends of varying slopes (0.1, 0.3, 0.5 per time unit) to test robustness. Methods that operate on differenced data (CFD-Autoperiod) should be more robust.

```{r}
#| label: challenge-trends

generate_with_trend <- function(base_data, slope) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        t <- seq_along(v) - 1
        v + slope * t
      }),
      scenario = paste0("trend_", slope)
    )
}

# Test with different trend slopes
# Use round() to handle floating-point comparison issues
trend_data <- bind_rows(
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.1),
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.5)
)

cat(sprintf("Generated %d curves with trends\n", nrow(trend_data)))
```

```{r}
#| label: fig-trend-examples
#| fig-cap: "Example curves with linear trends of varying slopes"
#| fig-height: 4

# Show examples of trend effects
example_curves <- baseline_data %>%
  filter(round(strength_level, 1) == 0.3) %>%
  slice(1)

example_with_trends <- bind_rows(
  example_curves %>% mutate(slope = "Original (no trend)", values = values),
  example_curves %>% mutate(slope = "Slope = 0.1", values = map(values, ~.x + 0.1 * (seq_along(.x) - 1))),
  example_curves %>% mutate(slope = "Slope = 0.3", values = map(values, ~.x + 0.3 * (seq_along(.x) - 1))),
  example_curves %>% mutate(slope = "Slope = 0.5", values = map(values, ~.x + 0.5 * (seq_along(.x) - 1)))
)

example_long <- example_with_trends %>%
  unnest(values) %>%
  group_by(slope) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(slope = factor(slope, levels = c("Original (no trend)", "Slope = 0.1", "Slope = 0.3", "Slope = 0.5")))

ggplot(example_long, aes(x = t, y = values, color = slope)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Effect of Linear Trend on Seasonal Signal",
    subtitle = "Seasonal strength = 0.3",
    x = "Time", y = "Value",
    color = "Trend"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 1b: Non-linear Trends

Beyond linear trends, real-world data often exhibits non-linear trend patterns. We test quadratic (accelerating/decelerating) and sinusoidal (cyclical) trends that can confuse period detection methods.

```{r}
#| label: challenge-nonlinear-trends

# Quadratic trend generator
generate_with_quadratic_trend <- function(base_data, curvature) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        t_norm <- (seq_along(v) - 1) / (n - 1)  # Normalize to 0-1
        v + curvature * t_norm^2
      }),
      scenario = paste0("quadratic_", curvature)
    )
}

# Sinusoidal trend generator (low-frequency trend overlaid on seasonal)
generate_with_sinusoidal_trend <- function(base_data, amplitude) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        t <- seq_along(v) - 1
        # Half-cycle sinusoidal trend (period = 2*n)
        v + amplitude * sin(pi * t / (n - 1))
      }),
      scenario = paste0("sinusoid_", amplitude)
    )
}

# Generate quadratic trend data
quadratic_data <- bind_rows(
  generate_with_quadratic_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 1),
  generate_with_quadratic_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 3),
  generate_with_quadratic_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 5)
)

# Generate sinusoidal trend data
sinusoid_data <- bind_rows(
  generate_with_sinusoidal_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.5),
  generate_with_sinusoidal_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 1.0),
  generate_with_sinusoidal_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 2.0)
)

cat(sprintf("Generated %d curves with quadratic trends\n", nrow(quadratic_data)))
cat(sprintf("Generated %d curves with sinusoidal trends\n", nrow(sinusoid_data)))
```

```{r}
#| label: fig-nonlinear-trend-examples
#| fig-cap: "Effect of non-linear trends on seasonal signal (strength = 0.3)"
#| fig-height: 5

# Get example curve
example_nonlinear <- baseline_data %>%
  filter(round(strength_level, 1) == 0.3) %>%
  slice(1)

n <- N_POINTS
t_norm <- (0:(n-1)) / (n-1)

nonlinear_examples <- bind_rows(
  example_nonlinear %>% mutate(trend_type = "Original", values = values),
  example_nonlinear %>% mutate(trend_type = "Quadratic (c=3)", values = map(values, ~.x + 3 * t_norm^2)),
  example_nonlinear %>% mutate(trend_type = "Sinusoidal (A=1.5)", values = map(values, ~.x + 1.5 * sin(pi * t_norm)))
) %>%
  unnest(values) %>%
  group_by(trend_type) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(trend_type = factor(trend_type, levels = c("Original", "Quadratic (c=3)", "Sinusoidal (A=1.5)")))

ggplot(nonlinear_examples, aes(x = t, y = values, color = trend_type)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Non-linear Trend Effects on Seasonal Signal",
    subtitle = "Seasonal strength = 0.3",
    x = "Time", y = "Value",
    color = "Trend Type"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 2: Red Noise (AR(1) Process)

**Why test Red Noise?** Financial and climate time series often exhibit autocorrelation (red noise), which can produce spurious spectral peaks that fool FFT-based methods into detecting false seasonality. This is a critical test for specificity—methods that "hallucinate" seasonality in autocorrelated noise will generate many false positives in real-world applications.

We replace white noise with AR(1) noise with coefficients $\phi$ = 0.0 (baseline), 0.3, 0.5, 0.7, and 0.9 (extreme). Higher $\phi$ means stronger autocorrelation.

```{r}
#| label: challenge-rednoise

generate_with_ar_noise <- function(base_data, phi) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        # Generate AR(1) noise
        ar_noise <- numeric(n)
        ar_noise[1] <- rnorm(1, 0, NOISE_SD)
        for (i in 2:n) {
          ar_noise[i] <- phi * ar_noise[i-1] + rnorm(1, 0, NOISE_SD * sqrt(1 - phi^2))
        }
        # Replace white noise component with AR noise
        seasonal <- v - rnorm(n, 0, NOISE_SD)  # Remove original noise
        seasonal + ar_noise
      }),
      scenario = paste0("ar_", phi)
    )
}

# Test with different AR coefficients
ar_data <- bind_rows(
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.0),  # White noise baseline
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.5),
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.7),
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.9)   # Extreme autocorrelation
)

cat(sprintf("Generated %d curves with AR(1) noise\n", nrow(ar_data)))
```

```{r}
#| label: fig-ar-examples
#| fig-cap: "Example curves with AR(1) colored noise (autocorrelated)"
#| fig-height: 4

set.seed(42)  # For reproducibility of AR examples

# Show examples of AR noise effects on non-seasonal curves
example_nonseasonal <- baseline_data %>%
  filter(round(strength_level, 1) == 0.0) %>%
  slice(1)

# Generate AR noise examples
generate_ar_noise <- function(n, phi) {
  noise <- numeric(n)
  noise[1] <- rnorm(1)
  for (i in 2:n) {
    noise[i] <- phi * noise[i-1] + sqrt(1 - phi^2) * rnorm(1)
  }
  noise * 0.3
}

ar_examples <- tibble(
  phi = c("White noise ($\\phi$=0)", "AR(1) $\\phi$=0.3", "AR(1) $\\phi$=0.5", "AR(1) $\\phi$=0.7", "AR(1) $\\phi$=0.9"),
  phi_val = c(0, 0.3, 0.5, 0.7, 0.9)
) %>%
  mutate(values = map(phi_val, ~generate_ar_noise(60, .x))) %>%
  unnest(values) %>%
  group_by(phi) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(phi = factor(phi, levels = c("White noise ($\\phi$=0)", "AR(1) $\\phi$=0.3", "AR(1) $\\phi$=0.5", "AR(1) $\\phi$=0.7", "AR(1) $\\phi$=0.9")))

ggplot(ar_examples, aes(x = t, y = values, color = phi)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "AR(1) Noise Patterns (Red Noise)",
    subtitle = "Higher autocorrelation produces smoother, potentially misleading patterns",
    x = "Time", y = "Value",
    color = "Noise Type"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 3: Outlier Contamination

**Why test Outliers?** Sensor data, financial time series, and operational metrics frequently contain point anomalies from equipment failures, data entry errors, or exceptional events. Robust seasonality detection methods should maintain accuracy despite these extreme values.

We inject outliers with probability 5-10% and magnitude 3-5 standard deviations. Methods using variance-based measures may be particularly vulnerable, while differencing-based methods may amplify outlier effects.

```{r}
#| label: challenge-outliers

generate_with_outliers <- function(base_data, outlier_prob, outlier_magnitude) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        outlier_idx <- runif(n) < outlier_prob
        v[outlier_idx] <- v[outlier_idx] + sample(c(-1, 1), sum(outlier_idx), replace = TRUE) *
                          outlier_magnitude * sd(v)
        v
      }),
      scenario = paste0("outliers_", outlier_prob, "_", outlier_magnitude)
    )
}

# Test with different outlier configurations
outlier_data <- bind_rows(
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.05, 3),
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.05, 5),
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.10, 3)
)

cat(sprintf("Generated %d curves with outliers\n", nrow(outlier_data)))
```

```{r}
#| label: fig-outlier-examples
#| fig-cap: "Example curves with outlier contamination at different levels"
#| fig-height: 4

set.seed(42)

# Show examples of outlier effects
example_seasonal <- baseline_data %>%
  filter(round(strength_level, 1) == 0.3) %>%
  slice(1)

base_vals <- example_seasonal$values[[1]]
n <- length(base_vals)

add_outliers <- function(v, prob, mag) {
  outlier_idx <- runif(length(v)) < prob
  v[outlier_idx] <- v[outlier_idx] + sample(c(-1, 1), sum(outlier_idx), replace = TRUE) * mag * sd(v)
  v
}

outlier_examples <- tibble(
  config = c("No outliers", "5% @ 3σ", "5% @ 5σ", "10% @ 3σ"),
  prob = c(0, 0.05, 0.05, 0.10),
  mag = c(0, 3, 5, 3)
) %>%
  rowwise() %>%
  mutate(values = list(if (prob == 0) base_vals else add_outliers(base_vals, prob, mag))) %>%
  ungroup() %>%
  unnest(values) %>%
  group_by(config) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(config = factor(config, levels = c("No outliers", "5% @ 3σ", "5% @ 5σ", "10% @ 3σ")))

ggplot(outlier_examples, aes(x = t, y = values, color = config)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Effect of Outlier Contamination on Seasonal Signal",
    subtitle = "Seasonal strength = 0.3",
    x = "Time", y = "Value",
    color = "Outlier Config"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 4: Shape Robustness

Real-world seasonality is rarely a perfect sinusoid. We test methods' ability to detect non-sinusoidal periodic patterns including square waves and triangle waves, which contain higher harmonic content.

```{r}
#| label: challenge-shapes

# Generate waveform helper
generate_waveform <- function(n_points, period, shape = "sine") {
  t <- 0:(n_points - 1)
  phase_in_period <- (t %% period) / period  # 0 to 1 within each period

  switch(shape,
    "sine" = sin(2 * pi * phase_in_period),
    "square" = ifelse(phase_in_period < 0.5, 1, -1),
    "triangle" = 4 * abs(phase_in_period - 0.5) - 1
  )
}

# Generate curves with different waveform shapes
generate_shape_curves <- function(n_per_level, n_points, period, noise_sd, shape,
                                   strength_levels = c(0.0, 0.3, 0.6)) {
  curves <- map_dfr(strength_levels, function(strength) {
    map_dfr(1:n_per_level, function(curve_idx) {
      # Amplitude from strength formula
      if (strength > 0 && strength < 1) {
        amplitude <- sqrt(strength * noise_sd^2 / (1 - strength))
      } else if (strength >= 1) {
        amplitude <- 10 * noise_sd
      } else {
        amplitude <- 0
      }

      # Generate waveform with random phase
      phase_shift <- sample(0:(period-1), 1)
      waveform <- generate_waveform(n_points, period, shape)
      seasonal <- amplitude * c(waveform[(phase_shift+1):n_points], waveform[1:phase_shift])
      noise <- rnorm(n_points, 0, noise_sd)
      values <- seasonal + noise

      tibble(
        curve_id = which(strength_levels == strength) * n_per_level + curve_idx,
        strength_level = strength,
        is_seasonal = strength >= STRENGTH_THRESHOLD,
        scenario = paste0("shape_", shape),
        values = list(values)
      )
    })
  })
  curves
}

# Generate curves for each waveform shape
set.seed(42)
shape_data <- bind_rows(
  generate_shape_curves(N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD, "sine"),
  generate_shape_curves(N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD, "square"),
  generate_shape_curves(N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD, "triangle")
)

cat(sprintf("Generated %d curves with different shapes\n", nrow(shape_data)))
```

```{r}
#| label: fig-shape-examples
#| fig-cap: "Seasonal waveform shapes (period = 12, normalized amplitude)"
#| fig-height: 4

shape_examples <- tibble(
  shape = c("Sine", "Square", "Triangle"),
  waveform = list(
    generate_waveform(60, 12, "sine"),
    generate_waveform(60, 12, "square"),
    generate_waveform(60, 12, "triangle")
  )
) %>%
  unnest(waveform) %>%
  group_by(shape) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(shape = factor(shape, levels = c("Sine", "Square", "Triangle")))

ggplot(shape_examples, aes(x = t, y = waveform, color = shape)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Seasonal Waveform Shapes",
    subtitle = "Square and triangle waves contain higher harmonics",
    x = "Time", y = "Amplitude",
    color = "Shape"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 5: Amplitude Modulation

Non-stationary seasonality with time-varying amplitude is common in real-world data (e.g., emerging or fading seasonal patterns). We test methods' robustness to amplitude modulation patterns.

```{r}
#| label: challenge-amplitude

# Amplitude modulation generator
generate_with_amplitude_modulation <- function(base_data, modulation_type) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        t_norm <- (seq_along(v) - 1) / (n - 1)  # Normalize to 0-1

        # Define modulation envelope
        envelope <- switch(modulation_type,
          "constant" = rep(1, n),                                    # Baseline
          "growth" = t_norm,                                         # 0 -> 1
          "decay" = 1 - t_norm,                                      # 1 -> 0
          "emergence" = ifelse(t_norm < 0.5, 0, 2 * (t_norm - 0.5))  # 0 first half, grow second
        )

        # Apply envelope - modulate around mean to preserve signal structure
        mean_v <- mean(v)
        (v - mean_v) * envelope + mean_v
      }),
      scenario = paste0("amplitude_", modulation_type)
    )
}

# Generate amplitude modulation data
amplitude_data <- bind_rows(
  generate_with_amplitude_modulation(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), "constant"),
  generate_with_amplitude_modulation(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), "growth"),
  generate_with_amplitude_modulation(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), "decay"),
  generate_with_amplitude_modulation(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), "emergence")
)

cat(sprintf("Generated %d curves with amplitude modulation\n", nrow(amplitude_data)))
```

```{r}
#| label: fig-amplitude-examples
#| fig-cap: "Amplitude modulation patterns applied to seasonal signal (strength = 0.5)"
#| fig-height: 6

# Get a seasonal example curve
example_amp <- baseline_data %>%
  filter(round(strength_level, 1) == 0.6) %>%
  slice(1)

n <- N_POINTS
t_norm <- (0:(n-1)) / (n-1)

# Apply different modulations
amplitude_examples <- bind_rows(
  example_amp %>% mutate(modulation = "Constant", values = values),
  example_amp %>% mutate(modulation = "Growth",
    values = map(values, function(v) {
      mean_v <- mean(v); (v - mean_v) * t_norm + mean_v
    })),
  example_amp %>% mutate(modulation = "Decay",
    values = map(values, function(v) {
      mean_v <- mean(v); (v - mean_v) * (1 - t_norm) + mean_v
    })),
  example_amp %>% mutate(modulation = "Emergence",
    values = map(values, function(v) {
      mean_v <- mean(v)
      envelope <- ifelse(t_norm < 0.5, 0, 2 * (t_norm - 0.5))
      (v - mean_v) * envelope + mean_v
    }))
) %>%
  unnest(values) %>%
  group_by(modulation) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(modulation = factor(modulation, levels = c("Constant", "Growth", "Decay", "Emergence")))

ggplot(amplitude_examples, aes(x = t, y = values, color = modulation)) +
  geom_line(linewidth = 0.6) +
  facet_wrap(~modulation, ncol = 2, scales = "free_y") +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Amplitude Modulation Patterns",
    subtitle = "Non-stationary seasonality scenarios",
    x = "Time", y = "Value"
  ) +
  theme(legend.position = "none")
```

### Challenge Scenario Performance

We evaluate all 13 methods on each challenge scenario, broken down by the specific challenge parameter to understand performance degradation.

```{r}
#| label: evaluate-challenges

# Helper to load data to DuckDB and get scores for ALL methods
get_challenge_scores <- function(challenge_data) {
  dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
  dbExecute(con, "
    CREATE TABLE challenge_curves (
      curve_id INTEGER,
      strength_level DOUBLE,
      is_seasonal BOOLEAN,
      scenario VARCHAR,
      values DOUBLE[]
    )
  ")

  for (i in 1:nrow(challenge_data)) {
    row <- challenge_data[i, ]
    values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")
    dbExecute(con, sprintf("
      INSERT INTO challenge_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
    ", row$curve_id, row$strength_level,
       ifelse(row$is_seasonal, "true", "false"),
       row$scenario, values_str))
  }

  tryCatch({
    dbGetQuery(con, "
      SELECT
          curve_id, strength_level, is_seasonal, scenario,
          -- Strength methods (with known period = 12)
          ts_seasonal_strength(values, 12, 'variance') as variance_score,
          ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
          ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
          -- Period detection methods
          (ts_aic_period(values)).r_squared as aic_score,
          LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
          (ts_estimate_period_acf(values)).confidence as acf_score,
          LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
          (ts_autoperiod(values)).acf_validation as autoperiod_score,
          (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
          1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
          (ts_matrix_profile_period(values)).confidence as mp_score,
          (ts_stl_period(values)).seasonal_strength as stl_score,
          (ts_ssa_period(values)).variance_explained as ssa_score
      FROM challenge_curves
    ")
  }, error = function(e) NULL)
}

# All methods for challenge evaluation
ALL_METHODS <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
                 "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

# Calculate metrics for a specific scenario subset
calc_scenario_metrics <- function(scores_df, methods = ALL_METHODS) {
  map_dfr(methods, function(m) {
    score_col <- paste0(m, "_score")
    if (!score_col %in% names(scores_df)) return(NULL)

    scores_vec <- scores_df[[score_col]]
    valid_idx <- !is.na(scores_vec) & !is.nan(scores_vec) & !is.infinite(scores_vec)

    if (sum(valid_idx) < 10 || length(unique(scores_df$is_seasonal[valid_idx])) < 2) {
      return(tibble(Method = method_labels[m], AUC = NA, TPR = NA, FPR = NA))
    }

    roc_obj <- tryCatch(
      roc(scores_df$is_seasonal[valid_idx], scores_vec[valid_idx], quiet = TRUE),
      error = function(e) NULL
    )

    if (is.null(roc_obj)) {
      return(tibble(Method = method_labels[m], AUC = NA, TPR = NA, FPR = NA))
    }

    auc_val <- as.numeric(auc(roc_obj))
    coords_best <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))

    tibble(
      Method = method_labels[m],
      AUC = auc_val,
      TPR = coords_best$sensitivity,
      FPR = 1 - coords_best$specificity
    )
  }) %>% compact() %>% bind_rows()
}
```

### Trend Robustness

```{r}
#| label: fig-trend-performance
#| fig-cap: "Method performance degradation with increasing trend slope"
#| fig-height: 6

trend_scores <- get_challenge_scores(trend_data)

trend_results <- trend_scores %>%
  mutate(slope = as.numeric(gsub("trend_", "", scenario))) %>%
  group_by(slope) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(trend_results, aes(x = factor(slope), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Trend Robustness: AUC vs Trend Slope",
    subtitle = "Lower AUC at higher slopes indicates sensitivity to trends",
    x = "Trend Slope", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

### Red Noise Robustness (AR(1) False Positive Rate)

For red noise tests, we focus on **non-seasonal curves only** to measure false positive rate (FPR) — i.e., how often methods incorrectly detect seasonality in autocorrelated noise.

```{r}
#| label: fig-ar-performance
#| fig-cap: "AUC with increasing AR(1) autocorrelation"
#| fig-height: 6

ar_scores <- get_challenge_scores(ar_data)

ar_results <- ar_scores %>%
  mutate(phi = as.numeric(gsub("ar_", "", scenario))) %>%
  group_by(phi) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(ar_results, aes(x = factor(phi), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Red Noise Robustness: AUC vs AR(1) Coefficient",
    subtitle = "Lower AUC at higher φ indicates sensitivity to autocorrelated noise",
    x = "AR(1) Coefficient (φ)", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

#### False Positive Rate by Noise Type

The key concern with red noise is false positives—detecting seasonality where none exists. We compare FPR on purely non-seasonal series with white noise ($\phi$=0) vs extreme red noise ($\phi$=0.9).

```{r}
#| label: fig-fpr-analysis
#| fig-cap: "False Positive Rates: Methods that 'hallucinate' seasonality show elevated FPR in red noise. A well-calibrated detector should maintain FPR at most 5% (dashed line) regardless of noise structure."
#| fig-height: 5

# Calculate FPR for non-seasonal series only
nonseasonal_ar <- ar_scores %>%
  filter(!is_seasonal) %>%
  mutate(phi = as.numeric(gsub("ar_", "", scenario)))

# Calculate FPR at phi=0 and phi=0.9 for each method
methods <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
             "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

fpr_data <- map_dfr(methods, function(m) {
  score_col <- paste0(m, "_score")
  method_label <- method_labels[m]

  # Get optimal threshold from baseline ROC (handle mp -> MatrixProfile mapping)
  idx <- which(sapply(roc_results, function(x) x$method == method_label))
  if (length(idx) == 0) return(NULL)
  roc_r <- roc_results[[idx[1]]]
  if (is.null(roc_r)) return(NULL)
  threshold <- roc_r$threshold

  # Calculate FPR at each phi level
  map_dfr(c(0.0, 0.9), function(phi_val) {
    subset <- nonseasonal_ar %>% filter(phi == phi_val)
    if (nrow(subset) == 0) return(NULL)

    scores_at_phi <- subset[[score_col]]
    valid_idx <- !is.na(scores_at_phi)
    if (sum(valid_idx) < 5) return(NULL)

    fp <- sum(scores_at_phi[valid_idx] >= threshold)
    total <- sum(valid_idx)

    tibble(
      Method = method_label,
      Scenario = if (phi_val == 0) "White Noise (phi=0)" else "Red Noise (phi=0.9)",
      FPR = fp / total
    )
  })
})

# Plot FPR comparison
plot_fpr_analysis(fpr_data, METHOD_COLORS)
```

### Outlier Robustness

```{r}
#| label: fig-outlier-performance
#| fig-cap: "Method performance under different outlier configurations"
#| fig-height: 5

outlier_scores <- get_challenge_scores(outlier_data)

outlier_results <- outlier_scores %>%
  mutate(
    config = gsub("outliers_", "", scenario),
    prob = as.numeric(gsub("_.*", "", config)),
    mag = as.numeric(gsub(".*_", "", config)),
    label = paste0(prob * 100, "% @ ", mag, "σ")
  ) %>%
  group_by(label) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup() %>%
  mutate(label = factor(label, levels = c("5% @ 3σ", "5% @ 5σ", "10% @ 3σ")))

ggplot(outlier_results, aes(x = label, y = AUC, fill = Method)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_manual(values = METHOD_COLORS) +
  labs(
    title = "Outlier Robustness: AUC by Contamination Level",
    subtitle = "Probability @ Magnitude format",
    x = "Outlier Configuration", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(nrow = 2))
```

### Non-linear Trend Robustness

```{r}
#| label: fig-quadratic-performance
#| fig-cap: "Method performance with increasing quadratic trend curvature"
#| fig-height: 6

quadratic_scores <- get_challenge_scores(quadratic_data)

quadratic_results <- quadratic_scores %>%
  mutate(curvature = as.numeric(gsub("quadratic_", "", scenario))) %>%
  group_by(curvature) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(quadratic_results, aes(x = factor(curvature), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Quadratic Trend Robustness",
    subtitle = "AUC vs quadratic curvature coefficient",
    x = "Curvature", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

```{r}
#| label: fig-sinusoid-performance
#| fig-cap: "Method performance with increasing sinusoidal trend amplitude"
#| fig-height: 6

sinusoid_scores <- get_challenge_scores(sinusoid_data)

sinusoid_results <- sinusoid_scores %>%
  mutate(amplitude = as.numeric(gsub("sinusoid_", "", scenario))) %>%
  group_by(amplitude) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(sinusoid_results, aes(x = factor(amplitude), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Sinusoidal Trend Robustness",
    subtitle = "AUC vs sinusoidal trend amplitude",
    x = "Amplitude", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

### Shape Robustness

```{r}
#| label: fig-shape-performance
#| fig-cap: "Method performance across waveform shapes"
#| fig-height: 6

shape_scores <- get_challenge_scores(shape_data)

shape_results <- shape_scores %>%
  mutate(shape = gsub("shape_", "", scenario)) %>%
  group_by(shape) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup() %>%
  mutate(shape = factor(shape, levels = c("sine", "square", "triangle"),
                        labels = c("Sine", "Square", "Triangle")))

ggplot(shape_results, aes(x = shape, y = AUC, fill = Method)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_manual(values = METHOD_COLORS) +
  labs(
    title = "Shape Robustness: AUC by Waveform Shape",
    subtitle = "Sine wave is baseline; square/triangle have higher harmonics",
    x = "Waveform Shape", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(nrow = 2))
```

### Amplitude Modulation Robustness

```{r}
#| label: fig-amplitude-performance
#| fig-cap: "Method performance under amplitude modulation"
#| fig-height: 6

amplitude_scores <- get_challenge_scores(amplitude_data)

amplitude_results <- amplitude_scores %>%
  mutate(modulation = gsub("amplitude_", "", scenario)) %>%
  group_by(modulation) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup() %>%
  mutate(modulation = factor(modulation,
    levels = c("constant", "growth", "decay", "emergence"),
    labels = c("Constant", "Growth", "Decay", "Emergence")))

ggplot(amplitude_results, aes(x = modulation, y = AUC, fill = Method)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_manual(values = METHOD_COLORS) +
  labs(
    title = "Amplitude Modulation Robustness",
    subtitle = "Tests non-stationary seasonality detection",
    x = "Modulation Type", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(nrow = 2))
```

### Challenge Summary Table

```{r}
#| label: challenge-summary

# Aggregate results across ALL challenges including new ones
all_challenge_results <- bind_rows(
  trend_results %>% mutate(Challenge = "Linear Trends", Parameter = as.character(slope)) %>% select(-slope),
  quadratic_results %>% mutate(Challenge = "Quadratic Trends", Parameter = as.character(curvature)) %>% select(-curvature),
  sinusoid_results %>% mutate(Challenge = "Sinusoidal Trends", Parameter = as.character(amplitude)) %>% select(-amplitude),
  ar_results %>% mutate(Challenge = "Red Noise (AR1)", Parameter = as.character(phi)) %>% select(-phi),
  outlier_results %>% mutate(Challenge = "Outliers", Parameter = label) %>% select(-label),
  shape_results %>% mutate(Challenge = "Shape Robustness", Parameter = as.character(shape)) %>% select(-shape),
  amplitude_results %>% mutate(Challenge = "Amplitude Modulation", Parameter = as.character(modulation)) %>% select(-modulation)
)

# Summary by challenge type
challenge_summary <- all_challenge_results %>%
  group_by(Challenge, Method) %>%
  summarise(
    `Mean AUC` = mean(AUC, na.rm = TRUE),
    `Min AUC` = min(AUC, na.rm = TRUE),
    `Max AUC` = max(AUC, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(Challenge, desc(`Mean AUC`))

kable(challenge_summary, digits = 3,
      caption = "Extended Challenge Scenario Summary: AUC Statistics by Method")
```

```{r}
#| label: fig-challenge-heatmap
#| fig-cap: "Challenge robustness heatmap: Mean AUC by method and challenge type"
#| fig-height: 7

challenge_heatmap_data <- all_challenge_results %>%
  group_by(Challenge, Method) %>%
  summarise(MeanAUC = mean(AUC, na.rm = TRUE), .groups = "drop")

ggplot(challenge_heatmap_data, aes(x = Challenge, y = Method, fill = MeanAUC)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = sprintf("%.2f", MeanAUC)), size = 2.5) +
  scale_fill_gradient2(low = "#d73027", mid = "#ffffbf", high = "#1a9850",
                       midpoint = 0.85, limits = c(0.5, 1)) +
  labs(
    title = "Challenge Robustness Heatmap",
    subtitle = "Mean AUC across challenge parameters",
    x = "Challenge Scenario", y = "Method", fill = "Mean AUC"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

\newpage

## M4 Real-World Validation

To validate synthetic benchmark results, we apply all detection methods to real-world time series from the M4 Competition Monthly dataset. Monthly series are expected to exhibit annual seasonality (period = 12).

```{r}
#| label: m4-load
#| cache: true

# Load M4 Monthly data via Python datasetsforecast
# Check if reticulate is available
reticulate_available <- requireNamespace("reticulate", quietly = TRUE)

m4_available <- FALSE
if (reticulate_available) {
  library(reticulate)
  # Check if datasetsforecast is available
  m4_available <- tryCatch({
    datasetsforecast <- import("datasetsforecast.m4")
    TRUE
  }, error = function(e) FALSE)
}

if (m4_available) {
  # Load M4 Monthly data
  m4_raw <- datasetsforecast$M4$load(
    directory = tempdir(),
    group = "Monthly"
  )

  m4_df <- as_tibble(m4_raw[[1]])

  # Aggregate to array format
  m4_prepared <- m4_df %>%
    group_by(unique_id) %>%
    summarise(
      values = list(y),
      n_obs = n(),
      .groups = "drop"
    ) %>%
    filter(n_obs >= 36)  # At least 3 years

  # Sample for tractable benchmark
  set.seed(42)
  m4_sample <- m4_prepared %>%
    slice_sample(n = min(1000, nrow(.)))

  cat(sprintf("Loaded %d M4 Monthly series (sampled %d)\n", nrow(m4_prepared), nrow(m4_sample)))
} else {
  if (!reticulate_available) {
    cat("Note: M4 validation skipped - reticulate R package not available\n")
    cat("Install with: install.packages('reticulate')\n")
  } else {
    cat("Note: M4 validation skipped - datasetsforecast Python package not available\n")
    cat("Install with: pip install datasetsforecast\n")
  }
  m4_sample <- NULL
}
```

```{r}
#| label: m4-scores
#| eval: !expr 'exists("m4_sample") && !is.null(m4_sample) && nrow(m4_sample) > 0'

# Load M4 sample to DuckDB
dbExecute(con, "DROP TABLE IF EXISTS m4_curves")
dbExecute(con, "
  CREATE TABLE m4_curves (
    series_id VARCHAR,
    n_obs INTEGER,
    values DOUBLE[]
  )
")

for (i in 1:nrow(m4_sample)) {
  row <- m4_sample[i, ]
  values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")
  dbExecute(con, sprintf("
    INSERT INTO m4_curves VALUES ('%s', %d, %s::DOUBLE[])
  ", row$unique_id, row$n_obs, values_str))
}

# Run all detection methods
m4_scores <- tryCatch({
  dbGetQuery(con, "
  SELECT
      series_id,
      n_obs,
      -- Strength methods (with expected period = 12)
      ts_seasonal_strength(values, 12, 'variance') as variance_score,
      ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
      ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
      -- Period detection methods
      (ts_aic_period(values)).r_squared as aic_score,
      (ts_aic_period(values)).period as aic_period,
      LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
      (ts_estimate_period_fft(values)).period as fft_period,
      (ts_estimate_period_acf(values)).confidence as acf_score,
      (ts_estimate_period_acf(values)).period as acf_period,
      LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
      (ts_autoperiod(values)).acf_validation as autoperiod_score,
      (ts_autoperiod(values)).period as autoperiod_period,
      (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
      (ts_cfd_autoperiod(values)).period as cfd_period,
      1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
      (ts_matrix_profile_period(values)).confidence as mp_score,
      (ts_stl_period(values)).seasonal_strength as stl_score,
      (ts_ssa_period(values)).variance_explained as ssa_score
  FROM m4_curves
  ")
}, error = function(e) {
  cat("Error extracting M4 scores:", e$message, "\n")
  NULL
})

if (!is.null(m4_scores)) {
  m4_scores <- as_tibble(m4_scores) %>%
    mutate(across(ends_with("_score"), ~ ifelse(is.nan(.) | is.infinite(.), NA_real_, .)))
  cat(sprintf("Computed scores for %d M4 series\n", nrow(m4_scores)))
}
```

```{r}
#| label: fig-m4-period-detection
#| fig-cap: "Distribution of detected periods on M4 Monthly data (expected = 12)"
#| fig-height: 6
#| eval: !expr 'exists("m4_scores") && !is.null(m4_scores)'

period_cols <- c("aic_period", "fft_period", "acf_period", "autoperiod_period", "cfd_period")

m4_periods <- m4_scores %>%
  select(series_id, all_of(period_cols)) %>%
  pivot_longer(cols = all_of(period_cols), names_to = "method", values_to = "detected_period") %>%
  mutate(
    method = gsub("_period", "", method),
    method = toupper(method),
    period_correct = abs(detected_period - 12) <= 1  # Within +/- 1 of 12
  )

# Calculate accuracy for each method
period_accuracy <- m4_periods %>%
  group_by(method) %>%
  summarise(
    accuracy = mean(period_correct, na.rm = TRUE),
    n_valid = sum(!is.na(detected_period))
  )

ggplot(m4_periods, aes(x = detected_period)) +
  geom_histogram(binwidth = 1, fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = 12, linetype = "dashed", color = "red", linewidth = 1) +
  facet_wrap(~method, ncol = 2, scales = "free_y") +
  scale_x_continuous(limits = c(0, 30), breaks = seq(0, 30, 6)) +
  labs(
    title = "Detected Periods on M4 Monthly Data",
    subtitle = "Red line = expected period (12)",
    x = "Detected Period", y = "Count"
  )
```

```{r}
#| label: fig-m4-detection-rate
#| fig-cap: "Seasonality detection rate on M4 Monthly data using optimal thresholds"
#| fig-height: 6
#| eval: !expr 'exists("m4_scores") && !is.null(m4_scores) && exists("auc_summary")'

# Use optimal thresholds from baseline analysis
method_thresholds <- auc_summary %>%
  select(Method, `Optimal Threshold`) %>%
  mutate(method_lower = tolower(gsub("MatrixProfile", "mp", Method)))

# Calculate detection rate for each method
m4_detection_rates <- map_dfr(1:nrow(method_thresholds), function(i) {
  method_name <- method_thresholds$Method[i]
  method_lower <- method_thresholds$method_lower[i]
  threshold <- method_thresholds$`Optimal Threshold`[i]
  score_col <- paste0(method_lower, "_score")

  if (score_col %in% names(m4_scores)) {
    scores <- m4_scores[[score_col]]
    detection_rate <- mean(scores >= threshold, na.rm = TRUE)
    tibble(Method = method_name, DetectionRate = detection_rate, n_valid = sum(!is.na(scores)))
  } else {
    NULL
  }
}) %>%
  filter(!is.null(Method))

ggplot(m4_detection_rates, aes(x = reorder(Method, DetectionRate), y = DetectionRate, fill = Method)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.1f%%", DetectionRate * 100)), hjust = -0.1, size = 3) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1.1), labels = scales::percent) +
  coord_flip() +
  labs(
    title = "M4 Monthly: Seasonality Detection Rate",
    subtitle = "Using optimal thresholds from baseline simulation",
    x = "", y = "Detection Rate"
  ) +
  theme(legend.position = "none")
```

```{r}
#| label: fig-m4-score-distributions
#| fig-cap: "Distribution of detection scores on M4 Monthly data"
#| fig-height: 7
#| eval: !expr 'exists("m4_scores") && !is.null(m4_scores)'

m4_scores_long <- m4_scores %>%
  select(series_id, ends_with("_score")) %>%
  pivot_longer(cols = ends_with("_score"), names_to = "method", values_to = "score") %>%
  mutate(
    method = gsub("_score", "", method),
    method = case_when(
      method == "aic" ~ "AIC", method == "fft" ~ "FFT", method == "acf" ~ "ACF",
      method == "variance" ~ "Variance", method == "spectral" ~ "Spectral",
      method == "wavelet" ~ "Wavelet", method == "sazed" ~ "SAZED",
      method == "autoperiod" ~ "Autoperiod", method == "cfd" ~ "CFD",
      method == "lomb" ~ "Lomb", method == "mp" ~ "MatrixProfile",
      method == "stl" ~ "STL", method == "ssa" ~ "SSA"
    ),
    method = factor(method, levels = names(METHOD_COLORS))
  )

ggplot(m4_scores_long, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.7, outlier.size = 0.5) +
  scale_fill_manual(values = METHOD_COLORS) +
  labs(
    title = "M4 Monthly: Score Distributions by Method",
    x = "", y = "Score"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
```

```{r}
#| label: m4-period-accuracy
#| eval: !expr 'exists("m4_periods")'

# Period detection accuracy table
if (exists("period_accuracy")) {
  kable(period_accuracy %>% arrange(desc(accuracy)),
        digits = 3,
        col.names = c("Method", "Period Accuracy (±1)", "Valid Series"),
        caption = "M4 Monthly: Period Detection Accuracy (detecting period 11-13)")
}
```

## Failure Analysis: Corner Cases

Understanding when the best-performing method fails provides actionable insights for practitioners. We analyze cases where the Variance method (highest F1) made incorrect predictions.

### False Negatives: Missed Seasonality

These are seasonal series that Variance failed to detect. False negatives are particularly costly in forecasting, as missing seasonality leads to systematically biased predictions.

```{r}
#| label: fig-failure-fn
#| fig-cap: "False Negative Examples: Seasonal patterns missed by Variance method. Common causes include low signal-to-noise ratio, irregular amplitudes, or dominant trends masking the seasonal component."
#| fig-height: 5

# Find false negatives for Variance method
variance_fn <- predictions %>%
  filter(is_seasonal & !variance_pred) %>%
  left_join(baseline_data %>% select(curve_id, values), by = "curve_id") %>%
  filter(!is.na(values))

if (nrow(variance_fn) > 0) {
  # Select 4 representative cases
  set.seed(42)
  fn_cases <- variance_fn %>%
    slice_sample(n = min(4, nrow(variance_fn)))

  plot_failure_cases(fn_cases, type = "fn", n_cases = 4)
} else {
  cat("No false negatives found for Variance method - perfect recall!")
}
```

### False Positives: Hallucinated Seasonality

These are non-seasonal series incorrectly classified as seasonal. While less critical than false negatives in forecasting, high false positive rates indicate the method may be over-sensitive.

```{r}
#| label: fig-failure-fp
#| fig-cap: "False Positive Examples: Non-seasonal series incorrectly classified as seasonal by Variance method. These cases often contain trends or autocorrelation patterns that mimic periodicity."
#| fig-height: 5

# Find false positives for Variance method
variance_fp <- predictions %>%
  filter(!is_seasonal & variance_pred) %>%
  left_join(baseline_data %>% select(curve_id, values), by = "curve_id") %>%
  filter(!is.na(values))

if (nrow(variance_fp) > 0) {
  # Select 4 representative cases
  set.seed(42)
  fp_cases <- variance_fp %>%
    slice_sample(n = min(4, nrow(variance_fp)))

  plot_failure_cases(fp_cases, type = "fp", n_cases = 4)
} else {
  cat("No false positives found for Variance method - perfect precision!")
}
```

\newpage

## Summary and Conclusions

### Final Rankings

```{r}
#| label: final-rankings

# Combine AUC from ROC analysis with F1 from classification
final_ranking <- auc_summary %>%
  left_join(metrics %>% select(Method, F1), by = "Method") %>%
  arrange(desc(F1)) %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Method, `ROC AUC`, `PR AUC`, F1, `Optimal Threshold`, Sensitivity, Specificity)

kable(final_ranking, digits = 3, caption = "Final Method Rankings by F1 Score")
```

### Key Findings

```{r}
#| label: key-findings

top_method <- final_ranking$Method[1]
top_f1 <- final_ranking$F1[1]
top_roc_auc <- final_ranking$`ROC AUC`[1]
top_pr_auc <- final_ranking$`PR AUC`[1]

cat(sprintf("**Best Overall Method**: %s (F1 = %.3f, ROC AUC = %.3f, PR AUC = %.3f)\n\n",
            top_method, top_f1, top_roc_auc, top_pr_auc))

# Methods with no significant difference from top
non_sig_from_top <- mcnemar_results %>%
  filter((method1 == top_method | method2 == top_method) & !significant) %>%
  mutate(other = ifelse(method1 == top_method, method2, method1)) %>%
  pull(other)

if (length(non_sig_from_top) > 0) {
  cat(sprintf("**Methods not significantly different from %s**: %s\n",
              top_method, paste(non_sig_from_top, collapse = ", ")))
}
```

### Recommendations

| Use Case | Recommended Method | Rationale |
|----------|-------------------|-----------|
| General detection | Wavelet or Variance | Highest F1 scores across challenges |
| Quick screening | FFT | Fast with good accuracy on clean data |
| Noisy data | ACF or Autoperiod | Robust to AR(1) autocorrelated noise |
| Data with trends | CFD-Autoperiod | Uses differencing to remove trends |
| Irregular sampling | Lomb-Scargle | Handles gaps and uneven spacing |
| Non-stationary amplitude | Wavelet or SSA | Time-frequency localization |
| Non-sinusoidal patterns | Autoperiod or ACF | Less sensitive to harmonic content |
| Emerging seasonality | Wavelet | Detects time-localized patterns |

## Cleanup

```{r}
#| label: cleanup
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
dbExecute(con, "DROP TABLE IF EXISTS m4_curves")
dbDisconnect(con, shutdown = TRUE)
```

## Session Info

```{r}
#| label: session-info

sessionInfo()
```
