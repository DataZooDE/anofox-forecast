---
title: "Seasonality Detection Methods: A Comparative Study"
subtitle: "Binary Classification Benchmark for the anofox-forecast DuckDB Extension"
author: "anofox-forecast benchmark suite"
date: today
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
    theme: cosmo
    fig-width: 10
    fig-height: 6
  pdf:
    toc: true
    toc-depth: 3
    colorlinks: true
    fig-width: 7
    fig-height: 5
execute:
  echo: false
  warning: false
  message: false
---

## Executive Summary

This benchmark evaluates seasonality detection methods as a **binary classification problem**: given a time series, does it contain seasonality? We simulate 550 curves with varying seasonal strength levels (0.0 to 1.0) and evaluate 13 detection methods using classification metrics (Accuracy, Precision, Recall, F1, ROC/AUC). Ground truth is defined as seasonal if simulated strength >= 0.2.

This benchmark replicates the methodology from the fdars R package benchmark.

```{r}
#| label: setup
#| cache: false

library(DBI)
library(duckdb)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(knitr)
library(scales)
library(pROC)

# Set ggplot theme
theme_set(theme_minimal(base_size = 12))

# Define consistent colors for all methods (13 total)
METHOD_COLORS <- c(
  "AIC" = "#E41A1C",
  "FFT" = "#377EB8",
  "ACF" = "#4DAF4A",
  "Variance" = "#984EA3",
  "Spectral" = "#FF7F00",
  "Wavelet" = "#FFFF33",
  "SAZED" = "#A65628",
  "Autoperiod" = "#F781BF",
  "CFD" = "#999999",
  "Lomb" = "#66C2A5",
  "MatrixProfile" = "#FC8D62",
  "STL" = "#8DA0CB",
  "SSA" = "#E78AC3"
)
```

## Introduction

### Detection as Binary Classification

Unlike period estimation (which asks "what is the period?"), **seasonality detection** asks a simpler question: **"is there seasonality?"** This is a binary classification problem where each method produces a confidence score, and we apply a threshold to make a detection decision.

### Methods Evaluated

| Method | SQL Function | Score Used | Description |
|--------|-------------|------------|-------------|
| AIC Comparison | `ts_aic_period` | R-squared | Fourier model fit quality |
| FFT Confidence | `ts_estimate_period_fft` | confidence | Peak-to-mean power ratio |
| ACF Confidence | `ts_estimate_period_acf` | confidence | Autocorrelation at lag |
| Variance Strength | `ts_seasonal_strength(..., 'variance')` | strength | Seasonal variance ratio |
| Spectral Strength | `ts_seasonal_strength(..., 'spectral')` | strength | Power at seasonal frequency |
| Wavelet Strength | `ts_seasonal_strength(..., 'wavelet')` | strength | Morlet wavelet energy |
| SAZED | `ts_sazed_period` | SNR | Zero-padded spectral SNR |
| Autoperiod | `ts_autoperiod` | acf_validation | FFT+ACF hybrid validation |
| CFD-Autoperiod | `ts_cfd_autoperiod` | acf_validation | First-differenced FFT+ACF |
| Lomb-Scargle | `ts_lomb_scargle` | 1-FAP | Statistical significance |
| Matrix Profile | `ts_matrix_profile_period` | confidence | Motif agreement ratio |
| STL | `ts_stl_period` | seasonal_strength | Decomposition strength |
| SSA | `ts_ssa_period` | variance_explained | Eigenvalue dominance |

### Ground Truth Definition

A series is classified as **seasonal** if its simulated seasonal strength >= 0.2. This threshold follows the fdars benchmark convention.

## Setup

### Connect to DuckDB and Load Extension

```{r}
#| label: connect-duckdb
#| cache: false

con <- dbConnect(duckdb(config = list("allow_unsigned_extensions" = "true")))

extension_path <- "../../build/release/extension/anofox_forecast/anofox_forecast.duckdb_extension"

if (file.exists(extension_path)) {
  dbExecute(con, sprintf("LOAD '%s'", extension_path))
  message("Extension loaded successfully!")
} else {
  warning("Extension not found at: ", extension_path,
          "\nPlease build the extension first with 'make' in the project root.")
}
```

## Baseline Simulation

### Simulation Parameters

Following the fdars benchmark:
- **11 strength levels**: 0.0, 0.1, 0.2, ..., 1.0
- **50 curves per level**: 550 total curves
- **60 observations**: 5 years of monthly data
- **Period = 12**: Monthly seasonality
- **White noise**: sigma = 0.3

```{r}
#| label: simulation-params

N_LEVELS <- 11
N_CURVES_PER_LEVEL <- 50
N_TOTAL <- N_LEVELS * N_CURVES_PER_LEVEL  # 550
N_POINTS <- 60
PERIOD <- 12.0
NOISE_SD <- 0.3
STRENGTH_THRESHOLD <- 0.2
SEED <- 42

set.seed(SEED)
```

### Baseline Data Generation

```{r}
#| label: generate-baseline

# Generate baseline curves with varying seasonal strength
generate_baseline <- function(n_levels, n_per_level, n_points, period, noise_sd) {
  strength_levels <- seq(0, 1, length.out = n_levels)

  curves <- map_dfr(seq_along(strength_levels), function(level_idx) {
    strength <- strength_levels[level_idx]

    map_dfr(1:n_per_level, function(curve_idx) {
      t <- 0:(n_points - 1)
      phase <- runif(1, 0, 2 * pi)

      # Amplitude derived from strength: strength = amp^2 / (amp^2 + noise^2)
      # So amp = sqrt(strength * noise^2 / (1 - strength)) when strength < 1
      if (strength > 0 && strength < 1) {
        amplitude <- sqrt(strength * noise_sd^2 / (1 - strength))
      } else if (strength >= 1) {
        amplitude <- 10 * noise_sd  # Very strong signal
      } else {
        amplitude <- 0
      }

      seasonal <- amplitude * sin(2 * pi * t / period + phase)
      noise <- rnorm(n_points, 0, noise_sd)
      values <- seasonal + noise

      tibble(
        curve_id = (level_idx - 1) * n_per_level + curve_idx,
        strength_level = strength,
        is_seasonal = strength >= STRENGTH_THRESHOLD,
        scenario = "baseline",
        values = list(values)
      )
    })
  })

  curves
}

baseline_data <- generate_baseline(N_LEVELS, N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD)

cat(sprintf("Generated %d curves\n", nrow(baseline_data)))
cat(sprintf("Seasonal (strength >= %.1f): %d\n", STRENGTH_THRESHOLD, sum(baseline_data$is_seasonal)))
cat(sprintf("Non-seasonal: %d\n", sum(!baseline_data$is_seasonal)))
```

### Strength Level Distribution

```{r}
#| label: fig-strength-dist
#| fig-cap: "Distribution of curves by seasonal strength level"

ggplot(baseline_data, aes(x = factor(strength_level), fill = is_seasonal)) +
  geom_bar() +
  scale_fill_manual(values = c("TRUE" = "#1a9850", "FALSE" = "#d73027"),
                    labels = c("TRUE" = "Seasonal", "FALSE" = "Non-seasonal")) +
  labs(
    title = "Curve Distribution by Strength Level",
    subtitle = sprintf("Ground truth: seasonal if strength >= %.1f", STRENGTH_THRESHOLD),
    x = "Seasonal Strength",
    y = "Number of Curves",
    fill = "Ground Truth"
  )
```

### Example Curves

```{r}
#| label: fig-example-curves
#| fig-cap: "Example curves at different strength levels"
#| fig-height: 8

examples <- baseline_data %>%
  filter(strength_level %in% c(0.0, 0.2, 0.5, 0.8, 1.0)) %>%
  group_by(strength_level) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(
    time = map(values, ~ 1:length(.x))
  ) %>%
  unnest(c(time, values))

ggplot(examples, aes(x = time, y = values)) +
  geom_line(color = "steelblue", linewidth = 0.5) +
  facet_wrap(~sprintf("Strength = %.1f", strength_level), ncol = 1, scales = "free_y") +
  labs(title = "Example Curves at Different Strength Levels", x = "Time", y = "Value")
```

### Load Data into DuckDB

```{r}
#| label: load-to-duckdb
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "
  CREATE TABLE benchmark_curves (
    curve_id INTEGER,
    strength_level DOUBLE,
    is_seasonal BOOLEAN,
    scenario VARCHAR,
    values DOUBLE[]
  )
")

for (i in 1:nrow(baseline_data)) {
  row <- baseline_data[i, ]
  values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")

  dbExecute(con, sprintf("
    INSERT INTO benchmark_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
  ", row$curve_id, row$strength_level,
     ifelse(row$is_seasonal, "true", "false"),
     row$scenario, values_str))
}

cat("Data loaded into DuckDB\n")
```

## Method Evaluation

### Extract Confidence Scores

```{r}
#| label: extract-scores
#| cache: false

# Run all detection methods and extract confidence scores
# Use known period = 12 for strength methods
scores <- dbGetQuery(con, "
SELECT
    curve_id,
    strength_level,
    is_seasonal,
    scenario,
    -- AIC: R-squared as confidence
    (ts_aic_period(values)).r_squared as aic_score,
    -- FFT: confidence (peak/mean power ratio), normalize to 0-1
    LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
    -- ACF: confidence (correlation value)
    (ts_estimate_period_acf(values)).confidence as acf_score,
    -- Strength methods (with known period = 12)
    ts_seasonal_strength(values, 12, 'variance') as variance_score,
    ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
    ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
    -- SAZED: SNR normalized
    LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
    -- Autoperiod: ACF validation
    (ts_autoperiod(values)).acf_validation as autoperiod_score,
    -- CFD-Autoperiod: ACF validation
    (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
    -- Lomb-Scargle: 1 - false alarm probability
    1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
    -- Matrix Profile: confidence
    (ts_matrix_profile_period(values)).confidence as mp_score,
    -- STL: seasonal strength
    (ts_stl_period(values)).seasonal_strength as stl_score,
    -- SSA: variance explained
    (ts_ssa_period(values)).variance_explained as ssa_score
FROM benchmark_curves
")

# Convert to tibble and handle NaN/Inf
scores <- as_tibble(scores) %>%
  mutate(across(ends_with("_score"), ~ ifelse(is.nan(.) | is.infinite(.), 0, .)))

cat(sprintf("Extracted scores for %d curves\n", nrow(scores)))
```

### Score Distributions by Ground Truth

```{r}
#| label: fig-score-distributions
#| fig-cap: "Distribution of confidence scores by ground truth (seasonal vs non-seasonal)"
#| fig-height: 12

score_long <- scores %>%
  pivot_longer(
    cols = ends_with("_score"),
    names_to = "method",
    values_to = "score"
  ) %>%
  mutate(
    method = gsub("_score", "", method),
    method = case_when(
      method == "aic" ~ "AIC",
      method == "fft" ~ "FFT",
      method == "acf" ~ "ACF",
      method == "variance" ~ "Variance",
      method == "spectral" ~ "Spectral",
      method == "wavelet" ~ "Wavelet",
      method == "sazed" ~ "SAZED",
      method == "autoperiod" ~ "Autoperiod",
      method == "cfd" ~ "CFD",
      method == "lomb" ~ "Lomb",
      method == "mp" ~ "MatrixProfile",
      method == "stl" ~ "STL",
      method == "ssa" ~ "SSA"
    ),
    method = factor(method, levels = names(METHOD_COLORS)),
    ground_truth = ifelse(is_seasonal, "Seasonal", "Non-seasonal")
  )

ggplot(score_long, aes(x = score, fill = ground_truth)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~method, scales = "free", ncol = 3) +
  scale_fill_manual(values = c("Seasonal" = "#1a9850", "Non-seasonal" = "#d73027")) +
  labs(
    title = "Confidence Score Distributions by Ground Truth",
    subtitle = "Good separation indicates discriminative power",
    x = "Confidence Score",
    y = "Density",
    fill = "Ground Truth"
  ) +
  theme(legend.position = "bottom")
```

## ROC Analysis

### Compute ROC Curves and AUC

```{r}
#| label: compute-roc

methods <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
             "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

method_labels <- c(
  "aic" = "AIC", "fft" = "FFT", "acf" = "ACF",
  "variance" = "Variance", "spectral" = "Spectral", "wavelet" = "Wavelet",
  "sazed" = "SAZED", "autoperiod" = "Autoperiod", "cfd" = "CFD",
  "lomb" = "Lomb", "mp" = "MatrixProfile", "stl" = "STL", "ssa" = "SSA"
)

# Compute ROC for each method
roc_results <- map(methods, function(m) {
  score_col <- paste0(m, "_score")
  score_vals <- scores[[score_col]]

  # Handle NA values
  valid_idx <- !is.na(score_vals)
  if (sum(valid_idx) < 10) return(NULL)

  roc_obj <- roc(scores$is_seasonal[valid_idx], score_vals[valid_idx],
                 quiet = TRUE, direction = "<")

  # Get optimal threshold (Youden's J)
  coords_best <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))

  list(
    method = method_labels[m],
    roc = roc_obj,
    auc = auc(roc_obj),
    threshold = coords_best$threshold,
    sensitivity = coords_best$sensitivity,
    specificity = coords_best$specificity
  )
}) %>%
  compact()

# Create AUC summary table
auc_summary <- map_dfr(roc_results, function(r) {
  tibble(
    Method = r$method,
    AUC = as.numeric(r$auc),
    `Optimal Threshold` = r$threshold,
    Sensitivity = r$sensitivity,
    Specificity = r$specificity
  )
}) %>%
  arrange(desc(AUC))

kable(auc_summary, digits = 3, caption = "ROC Analysis Summary (sorted by AUC)")
```

### ROC Curves

```{r}
#| label: fig-roc-curves
#| fig-cap: "ROC curves for all detection methods"
#| fig-height: 8

# Extract ROC curve data for plotting
roc_data <- map_dfr(roc_results, function(r) {
  tibble(
    method = r$method,
    sensitivity = r$roc$sensitivities,
    specificity = r$roc$specificities,
    fpr = 1 - r$roc$specificities,
    auc = as.numeric(r$auc)
  )
})

ggplot(roc_data, aes(x = fpr, y = sensitivity, color = method)) +
  geom_line(linewidth = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "ROC Curves for Seasonality Detection Methods",
    subtitle = "Diagonal line = random classifier",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Method"
  ) +
  coord_fixed() +
  theme(legend.position = "right")
```

### AUC Comparison

```{r}
#| label: fig-auc-comparison
#| fig-cap: "AUC comparison across methods"

ggplot(auc_summary, aes(x = reorder(Method, AUC), y = AUC, fill = Method)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.3f", AUC)), hjust = -0.1, size = 3) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1.1)) +
  coord_flip() +
  labs(
    title = "Area Under ROC Curve (AUC) by Method",
    subtitle = "Higher AUC = better discrimination",
    x = "",
    y = "AUC"
  ) +
  theme(legend.position = "none")
```

## Classification Performance

### Apply Optimal Thresholds

```{r}
#| label: apply-thresholds

# Create predictions using optimal thresholds
predictions <- scores %>%
  select(curve_id, is_seasonal)

for (r in roc_results) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  score_col <- paste0(method_lower, "_score")
  pred_col <- paste0(method_lower, "_pred")

  if (score_col %in% names(scores)) {
    predictions[[pred_col]] <- scores[[score_col]] >= r$threshold
  }
}

# Handle any remaining NAs
predictions <- predictions %>%
  mutate(across(ends_with("_pred"), ~ replace_na(., FALSE)))
```

### Classification Metrics

```{r}
#| label: classification-metrics

calc_metrics <- function(predicted, actual) {
  predicted <- replace(predicted, is.na(predicted), FALSE)
  actual <- replace(actual, is.na(actual), FALSE)

  tp <- sum(predicted & actual)
  tn <- sum(!predicted & !actual)
  fp <- sum(predicted & !actual)
  fn <- sum(!predicted & actual)

  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  specificity <- ifelse(tn + fp > 0, tn / (tn + fp), 0)
  fpr <- ifelse(fp + tn > 0, fp / (fp + tn), 0)
  f1 <- ifelse(precision + recall > 0, 2 * precision * recall / (precision + recall), 0)

  tibble(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    Specificity = specificity,
    FPR = fpr,
    F1 = f1
  )
}

# Calculate metrics for each method
metrics_list <- map(roc_results, function(r) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  pred_col <- paste0(method_lower, "_pred")

  if (pred_col %in% names(predictions)) {
    calc_metrics(predictions[[pred_col]], predictions$is_seasonal) %>%
      mutate(Method = r$method)
  } else {
    NULL
  }
}) %>%
  compact()

metrics <- bind_rows(metrics_list) %>%
  select(Method, Accuracy, Precision, Recall, Specificity, FPR, F1) %>%
  arrange(desc(F1))

kable(metrics, digits = 3, caption = "Classification Performance at Optimal Thresholds (sorted by F1)")
```

### Performance Comparison

```{r}
#| label: fig-performance-comparison
#| fig-cap: "Classification metrics comparison across methods"
#| fig-height: 8

metrics_long <- metrics %>%
  pivot_longer(cols = c(Accuracy, Precision, Recall, F1),
               names_to = "Metric", values_to = "Value")

ggplot(metrics_long, aes(x = Method, y = Value, fill = Method)) +
  geom_col() +
  facet_wrap(~Metric, ncol = 2) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1), labels = percent) +
  labs(
    title = "Classification Performance Metrics by Method",
    x = "",
    y = "Score"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )
```

## Statistical Significance: McNemar Tests

McNemar's test compares paired binary predictions between methods. A significant p-value indicates methods differ in their detection decisions.

```{r}
#| label: mcnemar-tests

run_mcnemar <- function(pred1, pred2) {
  pred1 <- replace(pred1, is.na(pred1), FALSE)
  pred2 <- replace(pred2, is.na(pred2), FALSE)

  b <- sum(pred1 & !pred2)  # Method1 positive, Method2 negative

  c <- sum(!pred1 & pred2)  # Method1 negative, Method2 positive

  if (b + c > 0) {
    chi_sq <- (abs(b - c) - 1)^2 / (b + c)
    p_value <- pchisq(chi_sq, df = 1, lower.tail = FALSE)
  } else {
    chi_sq <- 0
    p_value <- 1
  }

  list(chi_sq = chi_sq, p_value = p_value, b = b, c = c)
}

# Get all prediction columns
pred_cols <- names(predictions)[grepl("_pred$", names(predictions))]
method_names <- gsub("_pred$", "", pred_cols)

# Run pairwise McNemar tests
mcnemar_results <- expand.grid(
  method1 = method_names,
  method2 = method_names,
  stringsAsFactors = FALSE
) %>%
  filter(method1 < method2) %>%
  rowwise() %>%
  mutate(
    test = list(run_mcnemar(
      predictions[[paste0(method1, "_pred")]],
      predictions[[paste0(method2, "_pred")]]
    )),
    chi_sq = test$chi_sq,
    p_value = test$p_value,
    significant = p_value < 0.05
  ) %>%
  ungroup() %>%
  select(-test) %>%
  mutate(
    method1 = method_labels[method1],
    method2 = method_labels[method2]
  )

# Show significant differences
significant_pairs <- mcnemar_results %>%
  filter(significant) %>%
  arrange(p_value)

if (nrow(significant_pairs) > 0) {
  kable(significant_pairs %>% select(-significant), digits = 4,
        caption = "Significant McNemar Test Results (p < 0.05)")
} else {
  cat("No significant differences found between any method pairs.\n")
}
```

### McNemar P-Value Heatmap

```{r}
#| label: fig-mcnemar-heatmap
#| fig-cap: "McNemar test p-values between method pairs (red = significant difference)"
#| fig-height: 8

# Create symmetric matrix
mcnemar_matrix <- mcnemar_results %>%
  select(method1, method2, p_value) %>%
  bind_rows(
    mcnemar_results %>%
      rename(method1 = method2, method2 = method1) %>%
      select(method1, method2, p_value)
  )

ggplot(mcnemar_matrix, aes(x = method1, y = method2, fill = p_value)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = ifelse(p_value < 0.001, "<.001", sprintf("%.3f", p_value))),
            size = 2.5) +
  scale_fill_gradient2(low = "#d73027", mid = "#ffffbf", high = "#1a9850",
                       midpoint = 0.5, limits = c(0, 1)) +
  labs(
    title = "McNemar Test P-Values Between Methods",
    subtitle = "Red = significant difference (p < 0.05)",
    x = "", y = "",
    fill = "p-value"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Challenge Scenarios

Following the fdars benchmark, we test method robustness under challenging conditions.

### Challenge 1: Linear Trends

```{r}
#| label: challenge-trends

generate_with_trend <- function(base_data, slope) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        t <- seq_along(v) - 1
        v + slope * t
      }),
      scenario = paste0("trend_", slope)
    )
}

# Test with different trend slopes
trend_data <- bind_rows(
  generate_with_trend(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.1),
  generate_with_trend(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_trend(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.5)
)

cat(sprintf("Generated %d curves with trends\n", nrow(trend_data)))
```

### Challenge 2: Red Noise (AR(1) Process)

```{r}
#| label: challenge-rednoise

generate_with_ar_noise <- function(base_data, phi) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        # Generate AR(1) noise
        ar_noise <- numeric(n)
        ar_noise[1] <- rnorm(1, 0, NOISE_SD)
        for (i in 2:n) {
          ar_noise[i] <- phi * ar_noise[i-1] + rnorm(1, 0, NOISE_SD * sqrt(1 - phi^2))
        }
        # Replace white noise component with AR noise
        seasonal <- v - rnorm(n, 0, NOISE_SD)  # Remove original noise
        seasonal + ar_noise
      }),
      scenario = paste0("ar_", phi)
    )
}

# Test with different AR coefficients
ar_data <- bind_rows(
  generate_with_ar_noise(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_ar_noise(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.5),
  generate_with_ar_noise(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.7)
)

cat(sprintf("Generated %d curves with AR(1) noise\n", nrow(ar_data)))
```

### Challenge 3: Outlier Contamination

```{r}
#| label: challenge-outliers

generate_with_outliers <- function(base_data, outlier_prob, outlier_magnitude) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        outlier_idx <- runif(n) < outlier_prob
        v[outlier_idx] <- v[outlier_idx] + sample(c(-1, 1), sum(outlier_idx), replace = TRUE) *
                          outlier_magnitude * sd(v)
        v
      }),
      scenario = paste0("outliers_", outlier_prob, "_", outlier_magnitude)
    )
}

# Test with different outlier configurations
outlier_data <- bind_rows(
  generate_with_outliers(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.05, 3),
  generate_with_outliers(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.05, 5),
  generate_with_outliers(baseline_data %>% filter(strength_level %in% c(0.0, 0.3, 0.6)), 0.10, 3)
)

cat(sprintf("Generated %d curves with outliers\n", nrow(outlier_data)))
```

### Challenge Scenario Performance

```{r}
#| label: evaluate-challenges

evaluate_challenge <- function(challenge_data, scenario_name) {
  # Load challenge data into DuckDB
  dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
  dbExecute(con, "
    CREATE TABLE challenge_curves (
      curve_id INTEGER,
      strength_level DOUBLE,
      is_seasonal BOOLEAN,
      scenario VARCHAR,
      values DOUBLE[]
    )
  ")

  for (i in 1:nrow(challenge_data)) {
    row <- challenge_data[i, ]
    values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")

    dbExecute(con, sprintf("
      INSERT INTO challenge_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
    ", row$curve_id, row$strength_level,
       ifelse(row$is_seasonal, "true", "false"),
       row$scenario, values_str))
  }

  # Get scores for key methods (known period = 12)
  challenge_scores <- dbGetQuery(con, "
    SELECT
        curve_id,
        strength_level,
        is_seasonal,
        scenario,
        ts_seasonal_strength(values, 12, 'variance') as variance_score,
        ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
        (ts_estimate_period_fft(values)).confidence / 100.0 as fft_score,
        (ts_estimate_period_acf(values)).confidence as acf_score
    FROM challenge_curves
  ")

  # Calculate metrics for each method
  map_dfr(c("variance", "wavelet", "fft", "acf"), function(m) {
    score_col <- paste0(m, "_score")
    scores_vec <- challenge_scores[[score_col]]
    valid_idx <- !is.na(scores_vec) & !is.nan(scores_vec)

    if (sum(valid_idx) < 10) {
      return(tibble(Method = method_labels[m], Scenario = scenario_name, AUC = NA, F1 = NA))
    }

    roc_obj <- tryCatch(
      roc(challenge_scores$is_seasonal[valid_idx], scores_vec[valid_idx], quiet = TRUE),
      error = function(e) NULL
    )

    if (is.null(roc_obj)) {
      return(tibble(Method = method_labels[m], Scenario = scenario_name, AUC = NA, F1 = NA))
    }

    auc_val <- as.numeric(auc(roc_obj))
    threshold <- coords(roc_obj, "best")$threshold
    preds <- scores_vec >= threshold

    metrics <- calc_metrics(preds[valid_idx], challenge_scores$is_seasonal[valid_idx])

    tibble(
      Method = method_labels[m],
      Scenario = scenario_name,
      AUC = auc_val,
      F1 = metrics$F1
    )
  })
}

# Evaluate challenges
challenge_results <- bind_rows(
  evaluate_challenge(trend_data, "Trends"),
  evaluate_challenge(ar_data, "Red Noise"),
  evaluate_challenge(outlier_data, "Outliers")
)

kable(challenge_results %>% arrange(Scenario, desc(AUC)), digits = 3,
      caption = "Method Performance Under Challenge Scenarios")
```

## Summary and Conclusions

### Final Rankings

```{r}
#| label: final-rankings

# Combine AUC from ROC analysis with F1 from classification
final_ranking <- auc_summary %>%
  left_join(metrics %>% select(Method, F1), by = "Method") %>%
  arrange(desc(F1)) %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Method, AUC, F1, `Optimal Threshold`, Sensitivity, Specificity)

kable(final_ranking, digits = 3, caption = "Final Method Rankings by F1 Score")
```

### Key Findings

```{r}
#| label: key-findings

top_method <- final_ranking$Method[1]
top_f1 <- final_ranking$F1[1]
top_auc <- final_ranking$AUC[1]

cat(sprintf("**Best Overall Method**: %s (F1 = %.3f, AUC = %.3f)\n\n", top_method, top_f1, top_auc))

# Methods with no significant difference from top
non_sig_from_top <- mcnemar_results %>%
  filter((method1 == top_method | method2 == top_method) & !significant) %>%
  mutate(other = ifelse(method1 == top_method, method2, method1)) %>%
  pull(other)

if (length(non_sig_from_top) > 0) {
  cat(sprintf("**Methods not significantly different from %s**: %s\n",
              top_method, paste(non_sig_from_top, collapse = ", ")))
}
```

### Recommendations

| Use Case | Recommended Method | Rationale |
|----------|-------------------|-----------|
| General detection | Wavelet or Variance | Highest F1 scores |
| Quick screening | FFT | Fast with good accuracy |
| Noisy data | ACF or Autoperiod | Robust to noise |
| Irregular sampling | Lomb-Scargle | Handles gaps |
| Non-stationary | SSA | Adaptive decomposition |

## Cleanup

```{r}
#| label: cleanup
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
dbDisconnect(con, shutdown = TRUE)
```

## Session Info

```{r}
#| label: session-info

sessionInfo()
```
