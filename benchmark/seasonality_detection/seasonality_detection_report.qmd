---
title: "Seasonality Detection Methods: A Comparative Study"
subtitle: "Binary Classification Benchmark for the anofox-forecast DuckDB Extension"
author: "anofox-forecast benchmark suite"
date: today
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
    theme: cosmo
  pdf:
    toc: true
    toc-depth: 3
    colorlinks: true
    keep-tex: false
execute:
  echo: false
  warning: false
  message: false
knitr:
  opts_chunk:
    fig.width: 10
    fig.height: 6
    out.width: "100%"
    fig.align: "center"
---

## Executive Summary

This benchmark evaluates seasonality detection methods as a **binary classification problem**: given a time series, does it contain seasonality? We simulate 550 curves with varying seasonal strength levels (0.0 to 1.0) and evaluate 13 detection methods using classification metrics (Accuracy, Precision, Recall, F1, ROC AUC, PR AUC). Ground truth is defined as seasonal if simulated strength >= 0.2.

**Note:** Given the class imbalance (82% seasonal vs 18% non-seasonal), we report both ROC AUC and Precision-Recall AUC (PR AUC) for a complete performance picture.

This benchmark replicates the methodology from the fdars R package benchmark.

### Quick Start: Which Method Should I Use?

| Your Situation | Recommended Method | SQL Example |
|----------------|-------------------|-------------|
| **General purpose** | Wavelet or Variance Strength | `ts_seasonal_strength(values, period, 'wavelet')` |
| **Unknown period** | Autoperiod | `(ts_autoperiod(values)).detected` |
| **Fast screening** | FFT | `(ts_estimate_period_fft(values)).confidence` |
| **Trending data** | CFD-Autoperiod | `(ts_cfd_autoperiod(values)).detected` |
| **Noisy data** | ACF or Autoperiod | `(ts_estimate_period_acf(values)).confidence` |
| **Irregular sampling** | Lomb-Scargle | `(ts_lomb_scargle(values)).false_alarm_prob` |
| **Need model fit** | AIC | `(ts_aic_period(values)).r_squared` |

### Quick SQL Example

```sql
-- From long format data (one row per observation):
-- Aggregate values per series, then detect seasonality
SELECT
    series_id,
    (ts_autoperiod(LIST(value ORDER BY date))).detected AS has_seasonality,
    (ts_autoperiod(LIST(value ORDER BY date))).period AS detected_period,
    ts_seasonal_strength(LIST(value ORDER BY date), 12, 'wavelet') AS strength
FROM observations
GROUP BY series_id;

-- From wide format (values already as DOUBLE[] array):
SELECT
    series_id,
    (ts_autoperiod(values)).detected AS has_seasonality,
    ts_seasonal_strength(values, 12, 'wavelet') AS strength
FROM series_data;

-- Threshold: strength > 0.3 typically indicates seasonality
```

### Key Results (TL;DR)

- **Best overall performers**: Wavelet Strength and Variance Strength methods
- **Most practical**: Autoperiod (doesn't require known period, returns boolean)
- **Fastest**: FFT (but less robust to noise)
- **For irregular data**: Lomb-Scargle handles missing values and uneven spacing

*For detailed methodology and analysis, continue reading below.*

```{r}
#| label: setup
#| cache: false

library(DBI)
library(duckdb)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(knitr)
library(scales)
library(pROC)
library(PRROC)

# Set ggplot theme
theme_set(theme_minimal(base_size = 12))

# Define consistent colors for all methods (13 total)
# Colors aligned with fdars package for consistency
METHOD_COLORS <- c(
  "AIC" = "#2166AC",
  "FFT" = "#B2182B",
  "ACF" = "#1B7837",
  "Variance" = "#E66101",
  "Spectral" = "#762A83",
  "Wavelet" = "#D95F02",
  "SAZED" = "#984EA3",
  "Autoperiod" = "#FF7F00",
  "CFD" = "#A65628",
  "Lomb" = "#66C2A5",
  "MatrixProfile" = "#FC8D62",
  "STL" = "#8DA0CB",
  "SSA" = "#E78AC3"
)
```

## Introduction

### Detection as Binary Classification

Unlike period estimation (which asks "what is the period?"), **seasonality detection** asks a simpler question: **"is there seasonality?"** This is a binary classification problem where each method produces a confidence score, and we apply a threshold to make a detection decision.

### Methods Evaluated

| Method | SQL Function | Score Used | Description |
|--------|-------------|------------|-------------|
| AIC Comparison | `ts_aic_period` | R-squared | Fourier model fit quality |
| FFT Confidence | `ts_estimate_period_fft` | confidence | Peak-to-mean power ratio |
| ACF Confidence | `ts_estimate_period_acf` | confidence | Autocorrelation at lag |
| Variance Strength | `ts_seasonal_strength(..., 'variance')` | strength | Seasonal variance ratio |
| Spectral Strength | `ts_seasonal_strength(..., 'spectral')` | strength | Power at seasonal frequency |
| Wavelet Strength | `ts_seasonal_strength(..., 'wavelet')` | strength | Morlet wavelet energy |
| SAZED | `ts_sazed_period` | SNR | Zero-padded spectral SNR |
| Autoperiod | `ts_autoperiod` | acf_validation | FFT+ACF hybrid validation |
| CFD-Autoperiod | `ts_cfd_autoperiod` | acf_validation | First-differenced FFT+ACF |
| Lomb-Scargle | `ts_lomb_scargle` | 1-FAP | Statistical significance |
| Matrix Profile | `ts_matrix_profile_period` | confidence | Motif agreement ratio |
| STL | `ts_stl_period` | seasonal_strength | Decomposition strength |
| SSA | `ts_ssa_period` | variance_explained | Eigenvalue dominance |

### Detailed Method Descriptions

#### Spectral Methods

**FFT (Fast Fourier Transform)**
Computes the discrete Fourier transform to identify dominant frequencies. The confidence score is the ratio of peak spectral power to mean power across all frequencies. Fast ($O(n \log n)$) but sensitive to noise and non-stationarity.

$$X[k] = \sum_{t=0}^{N-1} x[t] \cdot e^{-2\pi ikt/N}, \quad \text{Confidence} = \frac{P[k_{max}]}{\bar{P}}$$

*Reference: Cooley, J.W. & Tukey, J.W. (1965). "An Algorithm for the Machine Calculation of Complex Fourier Series." Mathematics of Computation, 19(90), 297-301.*

**Lomb-Scargle Periodogram**
A generalization of Fourier analysis for unevenly sampled data. Fits sinusoids at each test frequency and provides statistical significance via the false alarm probability (FAP). Robust for irregular sampling.

$$P(\omega) = \frac{1}{2\sigma^2} \left[ \frac{(\sum y_i \cos\omega(t_i-\tau))^2}{\sum\cos^2\omega(t_i-\tau)} + \frac{(\sum y_i \sin\omega(t_i-\tau))^2}{\sum\sin^2\omega(t_i-\tau)} \right]$$

*References: Lomb, N.R. (1976). "Least-squares frequency analysis of unequally spaced data." Astrophysics and Space Science, 39, 447-462. Scargle, J.D. (1982). "Studies in astronomical time series analysis II." The Astrophysical Journal, 263, 835-853.*

**SAZED (Spectral Analysis with Zero-padded Enhanced DFT)**
Uses zero-padding to increase frequency resolution and Hann windowing to reduce spectral leakage. The signal-to-noise ratio (SNR) provides a confidence measure.

*Reference: Ding, H., et al. (2008). "Querying and Mining of Time Series Data." VLDB Endowment, 1(2), 1542-1552.*

#### Autocorrelation Methods

**ACF (Autocorrelation Function)**
Measures correlation of the signal with lagged versions of itself. Peaks in the ACF indicate periodic structure. The confidence is the ACF value at the detected period lag.

$$\text{ACF}(k) = \frac{\sum_{t=1}^{n-k} (x_t - \mu)(x_{t+k} - \mu)}{\sum_{t=1}^{n} (x_t - \mu)^2}$$

*Reference: Box, G.E.P. & Jenkins, G.M. (1976). Time Series Analysis: Forecasting and Control. Holden-Day.*

**Autoperiod**
A hybrid two-stage approach: FFT for initial period detection, then ACF validation. Combines spectral speed with time-domain robustness.

*Reference: Vlachos, M., Yu, P., & Castelli, V. (2005). "On Periodicity Detection and Structural Periodic Similarity." SIAM International Conference on Data Mining.*

**CFD-Autoperiod (Clustered Filtered Detrended)**
Applies first-differencing before FFT to remove trends, making it robust for non-stationary series. Validates with ACF on the original series.

*Reference: Elfeky, M.G., Aref, W.G., & Elmagarmid, A.K. (2005). "Periodicity Detection in Time Series Databases." IEEE TKDE, 17(7), 875-887.*

#### Model-Based Methods

**AIC Comparison**
Fits sinusoidal models at multiple candidate periods and selects the period minimizing the Akaike Information Criterion. Returns R² as a measure of model fit quality.

$$\text{AIC} = n \cdot \ln(\text{RSS}/n) + 2k, \quad R^2 = 1 - \frac{\text{RSS}}{\text{SS}_{total}}$$

*Reference: Akaike, H. (1974). "A new look at the statistical model identification." IEEE Transactions on Automatic Control, 19(6), 716-723.*

#### Decomposition Methods

**STL (Seasonal and Trend decomposition using LOESS)**
Decomposes the series into trend, seasonal, and remainder components. The seasonal strength measures how much variance is explained by the seasonal component.

$$F_S = \max\left(0, 1 - \frac{\text{Var}(R)}{\text{Var}(S + R)}\right)$$

*Reference: Cleveland, R.B., et al. (1990). "STL: A Seasonal-Trend Decomposition Procedure Based on Loess." Journal of Official Statistics, 6(1), 3-73.*

**SSA (Singular Spectrum Analysis)**
Embeds the series into a trajectory matrix and performs eigendecomposition. Periodic components appear as paired eigenvalues. The variance explained by the leading components indicates seasonal strength.

*Reference: Golyandina, N., Nekrutkin, V., & Zhigljavsky, A. (2001). Analysis of Time Series Structure: SSA and Related Techniques. Chapman & Hall/CRC.*

#### Strength-Based Methods

**Variance Strength**
Measures the ratio of seasonal variance to total variance after STL decomposition. Values near 1 indicate strong seasonality.

**Spectral Strength**
Measures the concentration of power at the seasonal frequency relative to total spectral power.

**Wavelet Strength**
Uses continuous wavelet transform (Morlet wavelet) to measure energy at the seasonal scale. Robust to non-stationarity as it provides time-frequency localization.

*Reference: Wang, X., Smith, K., & Hyndman, R. (2006). "Characteristic-based clustering for time series data." Data Mining and Knowledge Discovery, 13(3), 335-364.*

#### Pattern-Based Methods

**Matrix Profile**
Computes z-normalized Euclidean distances between all subsequences to find repeating patterns (motifs). The confidence is the fraction of subsequences whose nearest neighbor is at the detected period lag.

$$d(i,j) = \sqrt{\sum(z_i - z_j)^2}, \quad \text{Period} = \arg\max_k H[k]$$

*References: Yeh, C.C.M., et al. (2016). "Matrix Profile I: All Pairs Similarity Joins for Time Series." IEEE ICDM. Yeh, C.C.M., et al. (2017). "Matrix Profile VI: Meaningful Multidimensional Motif Discovery." IEEE ICDM.*

### Ground Truth Definition

A series is classified as **seasonal** if its simulated seasonal strength >= 0.2. This threshold follows the fdars benchmark convention.

## Setup

### Connect to DuckDB and Load Extension

```{r}
#| label: connect-duckdb
#| cache: false

con <- dbConnect(duckdb(config = list("allow_unsigned_extensions" = "true")))

extension_path <- "../../build/release/extension/anofox_forecast/anofox_forecast.duckdb_extension"

if (file.exists(extension_path)) {
  dbExecute(con, sprintf("LOAD '%s'", extension_path))
  message("Extension loaded successfully!")
} else {
  warning("Extension not found at: ", extension_path,
          "\nPlease build the extension first with 'make' in the project root.")
}
```

## Baseline Simulation

### Simulation Parameters

Following the fdars benchmark:
- **11 strength levels**: 0.0, 0.1, 0.2, ..., 1.0
- **50 curves per level**: 550 total curves
- **60 observations**: 5 years of monthly data
- **Period = 12**: Monthly seasonality
- **White noise**: sigma = 0.3

```{r}
#| label: simulation-params

N_LEVELS <- 11
N_CURVES_PER_LEVEL <- 50
N_TOTAL <- N_LEVELS * N_CURVES_PER_LEVEL  # 550
N_POINTS <- 60
PERIOD <- 12.0
NOISE_SD <- 0.3
STRENGTH_THRESHOLD <- 0.2
SEED <- 42

set.seed(SEED)
```

### Baseline Data Generation

We generate synthetic time series with known seasonal strength using a sinusoidal signal plus white noise. The amplitude is calibrated so that the signal-to-noise ratio corresponds to the target strength level: $\text{strength} = A^2 / (A^2 + \sigma^2)$.

```{r}
#| label: generate-baseline

# Generate baseline curves with varying seasonal strength
generate_baseline <- function(n_levels, n_per_level, n_points, period, noise_sd) {
  strength_levels <- seq(0, 1, length.out = n_levels)

  curves <- map_dfr(seq_along(strength_levels), function(level_idx) {
    strength <- strength_levels[level_idx]

    map_dfr(1:n_per_level, function(curve_idx) {
      t <- 0:(n_points - 1)
      phase <- runif(1, 0, 2 * pi)

      # Amplitude derived from strength: strength = amp^2 / (amp^2 + noise^2)
      # So amp = sqrt(strength * noise^2 / (1 - strength)) when strength < 1
      if (strength > 0 && strength < 1) {
        amplitude <- sqrt(strength * noise_sd^2 / (1 - strength))
      } else if (strength >= 1) {
        amplitude <- 10 * noise_sd  # Very strong signal
      } else {
        amplitude <- 0
      }

      seasonal <- amplitude * sin(2 * pi * t / period + phase)
      noise <- rnorm(n_points, 0, noise_sd)
      values <- seasonal + noise

      tibble(
        curve_id = (level_idx - 1) * n_per_level + curve_idx,
        strength_level = strength,
        is_seasonal = strength >= STRENGTH_THRESHOLD,
        scenario = "baseline",
        values = list(values)
      )
    })
  })

  curves
}

baseline_data <- generate_baseline(N_LEVELS, N_CURVES_PER_LEVEL, N_POINTS, PERIOD, NOISE_SD)

cat(sprintf("Generated %d curves\n", nrow(baseline_data)))
cat(sprintf("Seasonal (strength >= %.1f): %d\n", STRENGTH_THRESHOLD, sum(baseline_data$is_seasonal)))
cat(sprintf("Non-seasonal: %d\n", sum(!baseline_data$is_seasonal)))
```

### Strength Level Distribution

```{r}
#| label: fig-strength-dist
#| fig-cap: "Distribution of curves by seasonal strength level"

ggplot(baseline_data, aes(x = factor(strength_level), fill = is_seasonal)) +
  geom_bar() +
  scale_fill_manual(values = c("TRUE" = "#1a9850", "FALSE" = "#d73027"),
                    labels = c("TRUE" = "Seasonal", "FALSE" = "Non-seasonal")) +
  labs(
    title = "Curve Distribution by Strength Level",
    subtitle = sprintf("Ground truth: seasonal if strength >= %.1f", STRENGTH_THRESHOLD),
    x = "Seasonal Strength",
    y = "Number of Curves",
    fill = "Ground Truth"
  )
```

### Example Curves

```{r}
#| label: fig-example-curves
#| fig-cap: "Example curves at different strength levels"
#| fig-height: 7

examples <- baseline_data %>%
  filter(strength_level %in% c(0.0, 0.2, 0.5, 0.8, 1.0)) %>%
  group_by(strength_level) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(
    time = map(values, ~ 1:length(.x))
  ) %>%
  unnest(c(time, values))

ggplot(examples, aes(x = time, y = values)) +
  geom_line(color = "steelblue", linewidth = 0.5) +
  facet_wrap(~sprintf("Strength = %.1f", strength_level), ncol = 1, scales = "free_y") +
  labs(title = "Example Curves at Different Strength Levels", x = "Time", y = "Value")
```

### Load Data into DuckDB

The simulated curves are loaded into a DuckDB table for analysis. Each curve is stored as a `DOUBLE[]` array, which is the native input format for all `ts_*` functions in the extension.

```{r}
#| label: load-to-duckdb
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "
  CREATE TABLE benchmark_curves (
    curve_id INTEGER,
    strength_level DOUBLE,
    is_seasonal BOOLEAN,
    scenario VARCHAR,
    values DOUBLE[]
  )
")

for (i in 1:nrow(baseline_data)) {
  row <- baseline_data[i, ]
  values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")

  dbExecute(con, sprintf("
    INSERT INTO benchmark_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
  ", row$curve_id, row$strength_level,
     ifelse(row$is_seasonal, "true", "false"),
     row$scenario, values_str))
}

cat("Data loaded into DuckDB\n")
```

## Method Evaluation

### SQL API Usage

The following examples demonstrate how to use the seasonality detection methods. All `ts_*` functions expect a `DOUBLE[]` array as input.

**Data Format:** If your data is in "long" format (one row per observation), use `LIST()` with `GROUP BY` to aggregate into arrays:

```sql
-- Long format: one row per observation
-- +----------+------------+-------+
-- | series_id| date       | value |
-- +----------+------------+-------+
-- | A        | 2020-01-01 | 10.5  |
-- | A        | 2020-02-01 | 12.3  |
-- | B        | 2020-01-01 | 5.2   |
-- +----------+------------+-------+

-- Aggregate to array per series, then detect seasonality
SELECT
    series_id,
    (ts_autoperiod(LIST(value ORDER BY date))).detected AS has_seasonality,
    (ts_autoperiod(LIST(value ORDER BY date))).period AS detected_period
FROM long_format_data
GROUP BY series_id;
```

**Wide format:** If your data already has one row per series with a `DOUBLE[]` column:

```sql
-- Wide format: one row per series with array column
-- +----------+---------------------------+
-- | series_id| values                    |
-- +----------+---------------------------+
-- | A        | [10.5, 12.3, 11.8, ...]   |
-- | B        | [5.2, 6.1, 4.8, ...]      |
-- +----------+---------------------------+

SELECT
    series_id,

    -- Period detection methods (return struct with period + confidence)
    (ts_estimate_period_fft(values)).period AS fft_period,
    (ts_estimate_period_fft(values)).confidence AS fft_confidence,

    (ts_estimate_period_acf(values)).period AS acf_period,
    (ts_estimate_period_acf(values)).confidence AS acf_confidence,

    -- Autoperiod methods (FFT + ACF validation)
    (ts_autoperiod(values)).period AS autoperiod_period,
    (ts_autoperiod(values)).detected AS autoperiod_detected,
    (ts_autoperiod(values)).acf_validation AS autoperiod_score,

    (ts_cfd_autoperiod(values)).period AS cfd_period,
    (ts_cfd_autoperiod(values)).acf_validation AS cfd_score,

    -- Model-based methods
    (ts_aic_period(values)).period AS aic_period,
    (ts_aic_period(values)).r_squared AS aic_r_squared,

    -- Spectral methods
    (ts_lomb_scargle(values)).period AS lomb_period,
    (ts_lomb_scargle(values)).false_alarm_prob AS lomb_fap,

    (ts_sazed_period(values)).period AS sazed_period,
    (ts_sazed_period(values)).snr AS sazed_snr,

    -- Decomposition methods
    (ts_stl_period(values)).period AS stl_period,
    (ts_stl_period(values)).seasonal_strength AS stl_strength,

    (ts_ssa_period(values)).period AS ssa_period,
    (ts_ssa_period(values)).variance_explained AS ssa_variance,

    -- Pattern-based methods
    (ts_matrix_profile_period(values)).period AS mp_period,
    (ts_matrix_profile_period(values)).confidence AS mp_confidence,

    -- Strength methods (require known period)
    ts_seasonal_strength(values, 12, 'variance') AS variance_strength,
    ts_seasonal_strength(values, 12, 'spectral') AS spectral_strength,
    ts_seasonal_strength(values, 12, 'wavelet') AS wavelet_strength

FROM wide_format_data;
```

### Extract Confidence Scores

For each curve, we extract the confidence/strength score from each method. Scores are normalized to [0, 1] where possible for fair comparison.

```{r}
#| label: extract-scores
#| cache: false

# Run all detection methods and extract confidence scores
# Use known period = 12 for strength methods
scores <- dbGetQuery(con, "
SELECT
    curve_id,
    strength_level,
    is_seasonal,
    scenario,
    -- AIC: R-squared as confidence
    (ts_aic_period(values)).r_squared as aic_score,
    -- FFT: confidence (peak/mean power ratio), normalize to 0-1
    LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
    -- ACF: confidence (correlation value)
    (ts_estimate_period_acf(values)).confidence as acf_score,
    -- Strength methods (with known period = 12)
    ts_seasonal_strength(values, 12, 'variance') as variance_score,
    ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
    ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
    -- SAZED: SNR normalized
    LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
    -- Autoperiod: ACF validation
    (ts_autoperiod(values)).acf_validation as autoperiod_score,
    -- CFD-Autoperiod: ACF validation
    (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
    -- Lomb-Scargle: 1 - false alarm probability
    1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
    -- Matrix Profile: confidence
    (ts_matrix_profile_period(values)).confidence as mp_score,
    -- STL: seasonal strength
    (ts_stl_period(values)).seasonal_strength as stl_score,
    -- SSA: variance explained
    (ts_ssa_period(values)).variance_explained as ssa_score
FROM benchmark_curves
")

# Convert to tibble and handle NaN/Inf
scores <- as_tibble(scores) %>%
  mutate(across(ends_with("_score"), ~ ifelse(is.nan(.) | is.infinite(.), 0, .)))

cat(sprintf("Extracted scores for %d curves\n", nrow(scores)))
```

### Score Distributions by Ground Truth

```{r}
#| label: fig-score-distributions
#| fig-cap: "Distribution of confidence scores by ground truth (seasonal vs non-seasonal)"
#| fig-height: 9

score_long <- scores %>%
  pivot_longer(
    cols = ends_with("_score"),
    names_to = "method",
    values_to = "score"
  ) %>%
  mutate(
    method = gsub("_score", "", method),
    method = case_when(
      method == "aic" ~ "AIC",
      method == "fft" ~ "FFT",
      method == "acf" ~ "ACF",
      method == "variance" ~ "Variance",
      method == "spectral" ~ "Spectral",
      method == "wavelet" ~ "Wavelet",
      method == "sazed" ~ "SAZED",
      method == "autoperiod" ~ "Autoperiod",
      method == "cfd" ~ "CFD",
      method == "lomb" ~ "Lomb",
      method == "mp" ~ "MatrixProfile",
      method == "stl" ~ "STL",
      method == "ssa" ~ "SSA"
    ),
    method = factor(method, levels = names(METHOD_COLORS)),
    ground_truth = ifelse(is_seasonal, "Seasonal", "Non-seasonal")
  )

ggplot(score_long, aes(x = score, fill = ground_truth)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~method, scales = "free", ncol = 3) +
  scale_fill_manual(values = c("Seasonal" = "#1a9850", "Non-seasonal" = "#d73027")) +
  labs(
    title = "Confidence Score Distributions by Ground Truth",
    subtitle = "Good separation indicates discriminative power",
    x = "Confidence Score",
    y = "Density",
    fill = "Ground Truth"
  ) +
  theme(legend.position = "bottom")
```

## ROC Analysis

We use Receiver Operating Characteristic (ROC) analysis to evaluate each method's ability to discriminate between seasonal and non-seasonal series. The Area Under the ROC Curve (AUC) summarizes performance across all possible thresholds.

Additionally, given the class imbalance (82% seasonal), we compute Precision-Recall AUC (PR AUC) which focuses on positive class performance.

```{r}
#| label: compute-roc

methods <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
             "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

method_labels <- c(
  "aic" = "AIC", "fft" = "FFT", "acf" = "ACF",
  "variance" = "Variance", "spectral" = "Spectral", "wavelet" = "Wavelet",
  "sazed" = "SAZED", "autoperiod" = "Autoperiod", "cfd" = "CFD",
  "lomb" = "Lomb", "mp" = "MatrixProfile", "stl" = "STL", "ssa" = "SSA"
)

# Compute ROC and PR curves for each method
roc_results <- map(methods, function(m) {
  score_col <- paste0(m, "_score")
  score_vals <- scores[[score_col]]

  # Handle NA values
  valid_idx <- !is.na(score_vals)
  if (sum(valid_idx) < 10) return(NULL)

  roc_obj <- roc(scores$is_seasonal[valid_idx], score_vals[valid_idx],
                 quiet = TRUE, direction = "<")

  # Get optimal threshold (Youden's J)
  coords_best <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))

  # Compute PR AUC using PRROC package
  # scores.class0 = scores for positive class (seasonal)
  # scores.class1 = scores for negative class (non-seasonal)
  pr_obj <- pr.curve(
    scores.class0 = score_vals[valid_idx & scores$is_seasonal[valid_idx]],
    scores.class1 = score_vals[valid_idx & !scores$is_seasonal[valid_idx]],
    curve = FALSE
  )

  list(
    method = method_labels[m],
    roc = roc_obj,
    auc = auc(roc_obj),
    pr_auc = pr_obj$auc.integral,
    threshold = coords_best$threshold,
    sensitivity = coords_best$sensitivity,
    specificity = coords_best$specificity
  )
}) %>%
  compact()

# Create AUC summary table
auc_summary <- map_dfr(roc_results, function(r) {
  tibble(
    Method = r$method,
    `ROC AUC` = as.numeric(r$auc),
    `PR AUC` = r$pr_auc,
    `Optimal Threshold` = r$threshold,
    Sensitivity = r$sensitivity,
    Specificity = r$specificity
  )
}) %>%
  arrange(desc(`ROC AUC`))

kable(auc_summary, digits = 3, caption = "ROC and PR Analysis Summary (sorted by ROC AUC)")
```

### ROC Curves

```{r}
#| label: fig-roc-curves
#| fig-cap: "ROC curves for all detection methods"
#| fig-height: 6

# Extract ROC curve data for plotting
roc_data <- map_dfr(roc_results, function(r) {
  tibble(
    method = r$method,
    sensitivity = r$roc$sensitivities,
    specificity = r$roc$specificities,
    fpr = 1 - r$roc$specificities,
    auc = as.numeric(r$auc)
  )
})

ggplot(roc_data, aes(x = fpr, y = sensitivity, color = method)) +
  geom_line(linewidth = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "ROC Curves for Seasonality Detection Methods",
    subtitle = "Diagonal line = random classifier",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Method"
  ) +
  coord_fixed() +
  theme(legend.position = "right")
```

### AUC Comparison

```{r}
#| label: fig-auc-comparison
#| fig-cap: "ROC AUC and PR AUC comparison across methods"
#| fig-height: 7

# Reshape for faceted plot
auc_long <- auc_summary %>%
  select(Method, `ROC AUC`, `PR AUC`) %>%
  pivot_longer(cols = c(`ROC AUC`, `PR AUC`), names_to = "Metric", values_to = "AUC") %>%
  mutate(Metric = factor(Metric, levels = c("ROC AUC", "PR AUC")))

ggplot(auc_long, aes(x = reorder(Method, AUC), y = AUC, fill = Method)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.3f", AUC)), hjust = -0.1, size = 2.5) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1.15)) +
  coord_flip() +
  facet_wrap(~Metric, ncol = 2) +
  labs(
    title = "Area Under Curve Comparison",
    subtitle = "ROC AUC: overall discrimination | PR AUC: performance on positive class (82% imbalance)",
    x = "",
    y = "AUC"
  ) +
  theme(legend.position = "none")
```

### Optimal Threshold Estimation

The **optimal classification threshold** for each method is determined using **Youden's J statistic** (Youden, 1950), which maximizes the sum of sensitivity and specificity:

$$J = \text{Sensitivity} + \text{Specificity} - 1 = \text{TPR} - \text{FPR}$$

The threshold that maximizes $J$ represents the point on the ROC curve farthest from the diagonal (random classifier), providing the best trade-off between detecting true seasonality (sensitivity) and avoiding false positives (specificity).

**Why Youden's J?**

- **Balanced approach**: Does not favor sensitivity over specificity
- **Threshold-independent**: Works for any score distribution
- **Standard practice**: Widely used in diagnostic test evaluation

Alternative threshold selection methods include:

- **Fixed sensitivity**: Set threshold to achieve target sensitivity (e.g., 0.90)
- **Cost-based**: Minimize misclassification cost when FP and FN have different costs
- **Prevalence-adjusted**: Account for class imbalance in threshold selection

The `pROC::coords()` function with `best` method implements Youden's J optimization.

**Reference**: Youden, W. J. (1950). Index for rating diagnostic tests. *Cancer*, 3(1), 32-35.

## Classification Performance

Using the optimal threshold from ROC analysis (Youden's J statistic), we convert continuous scores into binary predictions and compute standard classification metrics.

```{r}
#| label: apply-thresholds

# Create predictions using optimal thresholds
predictions <- scores %>%
  select(curve_id, is_seasonal)

for (r in roc_results) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  score_col <- paste0(method_lower, "_score")
  pred_col <- paste0(method_lower, "_pred")

  if (score_col %in% names(scores)) {
    predictions[[pred_col]] <- scores[[score_col]] >= r$threshold
  }
}

# Handle any remaining NAs
predictions <- predictions %>%
  mutate(across(ends_with("_pred"), ~ replace_na(., FALSE)))
```

We calculate Accuracy, Precision (positive predictive value), Recall (sensitivity), Specificity, False Positive Rate, and F1 score for each method.

```{r}
#| label: classification-metrics

calc_metrics <- function(predicted, actual) {
  predicted <- replace(predicted, is.na(predicted), FALSE)
  actual <- replace(actual, is.na(actual), FALSE)

  tp <- sum(predicted & actual)
  tn <- sum(!predicted & !actual)
  fp <- sum(predicted & !actual)
  fn <- sum(!predicted & actual)

  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
  specificity <- ifelse(tn + fp > 0, tn / (tn + fp), 0)
  fpr <- ifelse(fp + tn > 0, fp / (fp + tn), 0)
  f1 <- ifelse(precision + recall > 0, 2 * precision * recall / (precision + recall), 0)

  tibble(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    Specificity = specificity,
    FPR = fpr,
    F1 = f1
  )
}

# Calculate metrics for each method
metrics_list <- map(roc_results, function(r) {
  method_lower <- tolower(gsub("MatrixProfile", "mp", r$method))
  pred_col <- paste0(method_lower, "_pred")

  if (pred_col %in% names(predictions)) {
    calc_metrics(predictions[[pred_col]], predictions$is_seasonal) %>%
      mutate(Method = r$method)
  } else {
    NULL
  }
}) %>%
  compact()

metrics <- bind_rows(metrics_list) %>%
  select(Method, Accuracy, Precision, Recall, Specificity, FPR, F1) %>%
  arrange(desc(F1))

kable(metrics, digits = 3, caption = "Classification Performance at Optimal Thresholds (sorted by F1)")
```

### Performance Comparison

```{r}
#| label: fig-performance-comparison
#| fig-cap: "Classification metrics comparison across methods"
#| fig-height: 6

metrics_long <- metrics %>%
  pivot_longer(cols = c(Accuracy, Precision, Recall, F1),
               names_to = "Metric", values_to = "Value")

ggplot(metrics_long, aes(x = Method, y = Value, fill = Method)) +
  geom_col() +
  facet_wrap(~Metric, ncol = 2) +
  scale_fill_manual(values = METHOD_COLORS) +
  scale_y_continuous(limits = c(0, 1), labels = percent) +
  labs(
    title = "Classification Performance Metrics by Method",
    x = "",
    y = "Score"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    legend.position = "none",
    plot.margin = margin(5, 15, 5, 5)
  )
```

## Statistical Significance: McNemar Tests

McNemar's test compares paired binary predictions between methods. A significant p-value indicates methods differ in their detection decisions.

```{r}
#| label: mcnemar-tests

run_mcnemar <- function(pred1, pred2) {
  pred1 <- replace(pred1, is.na(pred1), FALSE)
  pred2 <- replace(pred2, is.na(pred2), FALSE)

  b <- sum(pred1 & !pred2)  # Method1 positive, Method2 negative

  c <- sum(!pred1 & pred2)  # Method1 negative, Method2 positive

  if (b + c > 0) {
    chi_sq <- (abs(b - c) - 1)^2 / (b + c)
    p_value <- pchisq(chi_sq, df = 1, lower.tail = FALSE)
  } else {
    chi_sq <- 0
    p_value <- 1
  }

  list(chi_sq = chi_sq, p_value = p_value, b = b, c = c)
}

# Get all prediction columns
pred_cols <- names(predictions)[grepl("_pred$", names(predictions))]
method_names <- gsub("_pred$", "", pred_cols)

# Run pairwise McNemar tests
mcnemar_results <- expand.grid(
  method1 = method_names,
  method2 = method_names,
  stringsAsFactors = FALSE
) %>%
  filter(method1 < method2) %>%
  rowwise() %>%
  mutate(
    test = list(run_mcnemar(
      predictions[[paste0(method1, "_pred")]],
      predictions[[paste0(method2, "_pred")]]
    )),
    chi_sq = test$chi_sq,
    p_value = test$p_value
  ) %>%
  ungroup() %>%
  select(-test) %>%
  # Apply Benjamini-Hochberg (FDR) correction for multiple testing
  # With 13 methods, we have C(13,2) = 78 pairwise comparisons
  mutate(
    p_adjusted = p.adjust(p_value, method = "BH"),
    significant = p_adjusted < 0.05,
    method1 = method_labels[method1],
    method2 = method_labels[method2]
  )

cat(sprintf("Note: p-values adjusted for %d pairwise comparisons using Benjamini-Hochberg (FDR) correction.\n\n",
            nrow(mcnemar_results)))

# Show significant differences
significant_pairs <- mcnemar_results %>%
  filter(significant) %>%
  arrange(p_adjusted)

if (nrow(significant_pairs) > 0) {
  kable(significant_pairs %>% select(method1, method2, chi_sq, p_value, p_adjusted),
        digits = 4, col.names = c("Method 1", "Method 2", "Chi-sq", "p (raw)", "p (adjusted)"),
        caption = "Significant McNemar Test Results (adjusted p < 0.05)")
} else {
  cat("No significant differences found between any method pairs after FDR correction.\n")
}
```

### McNemar P-Value Heatmap (FDR-Adjusted)

```{r}
#| label: fig-mcnemar-heatmap
#| fig-cap: "McNemar test p-values (FDR-adjusted) between method pairs (red = significant difference)"
#| fig-height: 7

# Create symmetric matrix with adjusted p-values
mcnemar_matrix <- mcnemar_results %>%
  select(method1, method2, p_adjusted) %>%
  bind_rows(
    mcnemar_results %>%
      rename(method1 = method2, method2 = method1) %>%
      select(method1, method2, p_adjusted)
  )

ggplot(mcnemar_matrix, aes(x = method1, y = method2, fill = p_adjusted)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = ifelse(p_adjusted < 0.001, "<.001", sprintf("%.3f", p_adjusted))),
            size = 2.5) +
  scale_fill_gradient2(low = "#d73027", mid = "#ffffbf", high = "#1a9850",
                       midpoint = 0.5, limits = c(0, 1)) +
  labs(
    title = "McNemar Test P-Values Between Methods",
    subtitle = "FDR-adjusted (Benjamini-Hochberg); Red = significant difference (adj. p < 0.05)",
    x = "", y = "",
    fill = "Adj. p-value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
    axis.text.y = element_text(size = 7),
    plot.margin = margin(5, 15, 5, 5)
  )
```

## Challenge Scenarios

Following the fdars benchmark, we test method robustness under challenging conditions.

### Challenge 1: Linear Trends

Linear trends are common in real-world data and can mask or mimic seasonality. We add trends of varying slopes (0.1, 0.3, 0.5 per time unit) to test robustness. Methods that operate on differenced data (CFD-Autoperiod) should be more robust.

```{r}
#| label: challenge-trends

generate_with_trend <- function(base_data, slope) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        t <- seq_along(v) - 1
        v + slope * t
      }),
      scenario = paste0("trend_", slope)
    )
}

# Test with different trend slopes
# Use round() to handle floating-point comparison issues
trend_data <- bind_rows(
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.1),
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_trend(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.5)
)

cat(sprintf("Generated %d curves with trends\n", nrow(trend_data)))
```

```{r}
#| label: fig-trend-examples
#| fig-cap: "Example curves with linear trends of varying slopes"
#| fig-height: 4

# Show examples of trend effects
example_curves <- baseline_data %>%
  filter(round(strength_level, 1) == 0.3) %>%
  slice(1)

example_with_trends <- bind_rows(
  example_curves %>% mutate(slope = "Original (no trend)", values = values),
  example_curves %>% mutate(slope = "Slope = 0.1", values = map(values, ~.x + 0.1 * (seq_along(.x) - 1))),
  example_curves %>% mutate(slope = "Slope = 0.3", values = map(values, ~.x + 0.3 * (seq_along(.x) - 1))),
  example_curves %>% mutate(slope = "Slope = 0.5", values = map(values, ~.x + 0.5 * (seq_along(.x) - 1)))
)

example_long <- example_with_trends %>%
  unnest(values) %>%
  group_by(slope) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(slope = factor(slope, levels = c("Original (no trend)", "Slope = 0.1", "Slope = 0.3", "Slope = 0.5")))

ggplot(example_long, aes(x = t, y = values, color = slope)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Effect of Linear Trend on Seasonal Signal",
    subtitle = "Seasonal strength = 0.3",
    x = "Time", y = "Value",
    color = "Trend"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 2: Red Noise (AR(1) Process)

Red noise (autocorrelated noise) can produce spurious peaks in spectral analysis that may be mistaken for seasonality. We replace white noise with AR(1) noise with coefficients $\phi$ = 0.3, 0.5, 0.7. Higher $\phi$ means stronger autocorrelation.

```{r}
#| label: challenge-rednoise

generate_with_ar_noise <- function(base_data, phi) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        # Generate AR(1) noise
        ar_noise <- numeric(n)
        ar_noise[1] <- rnorm(1, 0, NOISE_SD)
        for (i in 2:n) {
          ar_noise[i] <- phi * ar_noise[i-1] + rnorm(1, 0, NOISE_SD * sqrt(1 - phi^2))
        }
        # Replace white noise component with AR noise
        seasonal <- v - rnorm(n, 0, NOISE_SD)  # Remove original noise
        seasonal + ar_noise
      }),
      scenario = paste0("ar_", phi)
    )
}

# Test with different AR coefficients
ar_data <- bind_rows(
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.3),
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.5),
  generate_with_ar_noise(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.7)
)

cat(sprintf("Generated %d curves with AR(1) noise\n", nrow(ar_data)))
```

```{r}
#| label: fig-ar-examples
#| fig-cap: "Example curves with AR(1) colored noise (autocorrelated)"
#| fig-height: 4

set.seed(42)  # For reproducibility of AR examples

# Show examples of AR noise effects on non-seasonal curves
example_nonseasonal <- baseline_data %>%
  filter(round(strength_level, 1) == 0.0) %>%
  slice(1)

# Generate AR noise examples
generate_ar_noise <- function(n, phi) {
  noise <- numeric(n)
  noise[1] <- rnorm(1)
  for (i in 2:n) {
    noise[i] <- phi * noise[i-1] + sqrt(1 - phi^2) * rnorm(1)
  }
  noise * 0.3
}

ar_examples <- tibble(
  phi = c("White noise ($\\phi$=0)", "AR(1) $\\phi$=0.3", "AR(1) $\\phi$=0.5", "AR(1) $\\phi$=0.7"),
  phi_val = c(0, 0.3, 0.5, 0.7)
) %>%
  mutate(values = map(phi_val, ~generate_ar_noise(60, .x))) %>%
  unnest(values) %>%
  group_by(phi) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(phi = factor(phi, levels = c("White noise ($\\phi$=0)", "AR(1) $\\phi$=0.3", "AR(1) $\\phi$=0.5", "AR(1) $\\phi$=0.7")))

ggplot(ar_examples, aes(x = t, y = values, color = phi)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "AR(1) Noise Patterns (Red Noise)",
    subtitle = "Higher autocorrelation produces smoother, potentially misleading patterns",
    x = "Time", y = "Value",
    color = "Noise Type"
  ) +
  theme(legend.position = "bottom")
```

### Challenge 3: Outlier Contamination

Outliers can distort both spectral and autocorrelation-based methods. We inject outliers with probability 5-10% and magnitude 3-5 standard deviations. Robust methods should maintain performance.

```{r}
#| label: challenge-outliers

generate_with_outliers <- function(base_data, outlier_prob, outlier_magnitude) {
  base_data %>%
    mutate(
      values = map(values, function(v) {
        n <- length(v)
        outlier_idx <- runif(n) < outlier_prob
        v[outlier_idx] <- v[outlier_idx] + sample(c(-1, 1), sum(outlier_idx), replace = TRUE) *
                          outlier_magnitude * sd(v)
        v
      }),
      scenario = paste0("outliers_", outlier_prob, "_", outlier_magnitude)
    )
}

# Test with different outlier configurations
outlier_data <- bind_rows(
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.05, 3),
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.05, 5),
  generate_with_outliers(baseline_data %>% filter(round(strength_level, 1) %in% c(0.0, 0.3, 0.6)), 0.10, 3)
)

cat(sprintf("Generated %d curves with outliers\n", nrow(outlier_data)))
```

```{r}
#| label: fig-outlier-examples
#| fig-cap: "Example curves with outlier contamination at different levels"
#| fig-height: 4

set.seed(42)

# Show examples of outlier effects
example_seasonal <- baseline_data %>%
  filter(round(strength_level, 1) == 0.3) %>%
  slice(1)

base_vals <- example_seasonal$values[[1]]
n <- length(base_vals)

add_outliers <- function(v, prob, mag) {
  outlier_idx <- runif(length(v)) < prob
  v[outlier_idx] <- v[outlier_idx] + sample(c(-1, 1), sum(outlier_idx), replace = TRUE) * mag * sd(v)
  v
}

outlier_examples <- tibble(
  config = c("No outliers", "5% @ 3σ", "5% @ 5σ", "10% @ 3σ"),
  prob = c(0, 0.05, 0.05, 0.10),
  mag = c(0, 3, 5, 3)
) %>%
  rowwise() %>%
  mutate(values = list(if (prob == 0) base_vals else add_outliers(base_vals, prob, mag))) %>%
  ungroup() %>%
  unnest(values) %>%
  group_by(config) %>%
  mutate(t = row_number() - 1) %>%
  ungroup() %>%
  mutate(config = factor(config, levels = c("No outliers", "5% @ 3σ", "5% @ 5σ", "10% @ 3σ")))

ggplot(outlier_examples, aes(x = t, y = values, color = config)) +
  geom_line(linewidth = 0.8) +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Effect of Outlier Contamination on Seasonal Signal",
    subtitle = "Seasonal strength = 0.3",
    x = "Time", y = "Value",
    color = "Outlier Config"
  ) +
  theme(legend.position = "bottom")
```

### Challenge Scenario Performance

We evaluate all 13 methods on each challenge scenario, broken down by the specific challenge parameter to understand performance degradation.

```{r}
#| label: evaluate-challenges

# Helper to load data to DuckDB and get scores for ALL methods
get_challenge_scores <- function(challenge_data) {
  dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
  dbExecute(con, "
    CREATE TABLE challenge_curves (
      curve_id INTEGER,
      strength_level DOUBLE,
      is_seasonal BOOLEAN,
      scenario VARCHAR,
      values DOUBLE[]
    )
  ")

  for (i in 1:nrow(challenge_data)) {
    row <- challenge_data[i, ]
    values_str <- paste0("[", paste(row$values[[1]], collapse = ","), "]")
    dbExecute(con, sprintf("
      INSERT INTO challenge_curves VALUES (%d, %f, %s, '%s', %s::DOUBLE[])
    ", row$curve_id, row$strength_level,
       ifelse(row$is_seasonal, "true", "false"),
       row$scenario, values_str))
  }

  tryCatch({
    dbGetQuery(con, "
      SELECT
          curve_id, strength_level, is_seasonal, scenario,
          -- Strength methods (with known period = 12)
          ts_seasonal_strength(values, 12, 'variance') as variance_score,
          ts_seasonal_strength(values, 12, 'spectral') as spectral_score,
          ts_seasonal_strength(values, 12, 'wavelet') as wavelet_score,
          -- Period detection methods
          (ts_aic_period(values)).r_squared as aic_score,
          LEAST(1.0, (ts_estimate_period_fft(values)).confidence / 100.0) as fft_score,
          (ts_estimate_period_acf(values)).confidence as acf_score,
          LEAST(1.0, (ts_sazed_period(values)).snr / 10.0) as sazed_score,
          (ts_autoperiod(values)).acf_validation as autoperiod_score,
          (ts_cfd_autoperiod(values)).acf_validation as cfd_score,
          1.0 - (ts_lomb_scargle(values)).false_alarm_prob as lomb_score,
          (ts_matrix_profile_period(values)).confidence as mp_score,
          (ts_stl_period(values)).seasonal_strength as stl_score,
          (ts_ssa_period(values)).variance_explained as ssa_score
      FROM challenge_curves
    ")
  }, error = function(e) NULL)
}

# All methods for challenge evaluation
ALL_METHODS <- c("aic", "fft", "acf", "variance", "spectral", "wavelet",
                 "sazed", "autoperiod", "cfd", "lomb", "mp", "stl", "ssa")

# Calculate metrics for a specific scenario subset
calc_scenario_metrics <- function(scores_df, methods = ALL_METHODS) {
  map_dfr(methods, function(m) {
    score_col <- paste0(m, "_score")
    if (!score_col %in% names(scores_df)) return(NULL)

    scores_vec <- scores_df[[score_col]]
    valid_idx <- !is.na(scores_vec) & !is.nan(scores_vec) & !is.infinite(scores_vec)

    if (sum(valid_idx) < 10 || length(unique(scores_df$is_seasonal[valid_idx])) < 2) {
      return(tibble(Method = method_labels[m], AUC = NA, TPR = NA, FPR = NA))
    }

    roc_obj <- tryCatch(
      roc(scores_df$is_seasonal[valid_idx], scores_vec[valid_idx], quiet = TRUE),
      error = function(e) NULL
    )

    if (is.null(roc_obj)) {
      return(tibble(Method = method_labels[m], AUC = NA, TPR = NA, FPR = NA))
    }

    auc_val <- as.numeric(auc(roc_obj))
    coords_best <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"))

    tibble(
      Method = method_labels[m],
      AUC = auc_val,
      TPR = coords_best$sensitivity,
      FPR = 1 - coords_best$specificity
    )
  }) %>% compact() %>% bind_rows()
}
```

### Trend Robustness

```{r}
#| label: fig-trend-performance
#| fig-cap: "Method performance degradation with increasing trend slope"
#| fig-height: 6

trend_scores <- get_challenge_scores(trend_data)

trend_results <- trend_scores %>%
  mutate(slope = as.numeric(gsub("trend_", "", scenario))) %>%
  group_by(slope) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(trend_results, aes(x = factor(slope), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Trend Robustness: AUC vs Trend Slope",
    subtitle = "Lower AUC at higher slopes indicates sensitivity to trends",
    x = "Trend Slope", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

### Red Noise Robustness (AR(1) False Positive Rate)

For red noise tests, we focus on **non-seasonal curves only** to measure false positive rate (FPR) — i.e., how often methods incorrectly detect seasonality in autocorrelated noise.

```{r}
#| label: fig-ar-performance
#| fig-cap: "AUC with increasing AR(1) autocorrelation"
#| fig-height: 6

ar_scores <- get_challenge_scores(ar_data)

ar_results <- ar_scores %>%
  mutate(phi = as.numeric(gsub("ar_", "", scenario))) %>%
  group_by(phi) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup()

ggplot(ar_results, aes(x = factor(phi), y = AUC, color = Method, group = Method)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Red Noise Robustness: AUC vs AR(1) Coefficient",
    subtitle = "Lower AUC at higher φ indicates sensitivity to autocorrelated noise",
    x = "AR(1) Coefficient (φ)", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2))
```

### Outlier Robustness

```{r}
#| label: fig-outlier-performance
#| fig-cap: "Method performance under different outlier configurations"
#| fig-height: 5

outlier_scores <- get_challenge_scores(outlier_data)

outlier_results <- outlier_scores %>%
  mutate(
    config = gsub("outliers_", "", scenario),
    prob = as.numeric(gsub("_.*", "", config)),
    mag = as.numeric(gsub(".*_", "", config)),
    label = paste0(prob * 100, "% @ ", mag, "σ")
  ) %>%
  group_by(label) %>%
  group_modify(~calc_scenario_metrics(.x)) %>%
  ungroup() %>%
  mutate(label = factor(label, levels = c("5% @ 3σ", "5% @ 5σ", "10% @ 3σ")))

ggplot(outlier_results, aes(x = label, y = AUC, fill = Method)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_manual(values = METHOD_COLORS) +
  labs(
    title = "Outlier Robustness: AUC by Contamination Level",
    subtitle = "Probability @ Magnitude format",
    x = "Outlier Configuration", y = "AUC"
  ) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(nrow = 2))
```

### Challenge Summary Table

```{r}
#| label: challenge-summary

# Aggregate results across all challenges
all_challenge_results <- bind_rows(
  trend_results %>% mutate(Challenge = "Trends", Parameter = as.character(slope)) %>% select(-slope),
  ar_results %>% mutate(Challenge = "Red Noise", Parameter = as.character(phi)) %>% select(-phi),
  outlier_results %>% mutate(Challenge = "Outliers", Parameter = label) %>% select(-label)
)

# Summary by challenge type
challenge_summary <- all_challenge_results %>%
  group_by(Challenge, Method) %>%
  summarise(
    `Mean AUC` = mean(AUC, na.rm = TRUE),
    `Min AUC` = min(AUC, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(Challenge, desc(`Mean AUC`))

kable(challenge_summary, digits = 3,
      caption = "Challenge Scenario Summary: Mean and Minimum AUC by Method")
```

## Summary and Conclusions

### Final Rankings

```{r}
#| label: final-rankings

# Combine AUC from ROC analysis with F1 from classification
final_ranking <- auc_summary %>%
  left_join(metrics %>% select(Method, F1), by = "Method") %>%
  arrange(desc(F1)) %>%
  mutate(Rank = row_number()) %>%
  select(Rank, Method, `ROC AUC`, `PR AUC`, F1, `Optimal Threshold`, Sensitivity, Specificity)

kable(final_ranking, digits = 3, caption = "Final Method Rankings by F1 Score")
```

### Key Findings

```{r}
#| label: key-findings

top_method <- final_ranking$Method[1]
top_f1 <- final_ranking$F1[1]
top_roc_auc <- final_ranking$`ROC AUC`[1]
top_pr_auc <- final_ranking$`PR AUC`[1]

cat(sprintf("**Best Overall Method**: %s (F1 = %.3f, ROC AUC = %.3f, PR AUC = %.3f)\n\n",
            top_method, top_f1, top_roc_auc, top_pr_auc))

# Methods with no significant difference from top
non_sig_from_top <- mcnemar_results %>%
  filter((method1 == top_method | method2 == top_method) & !significant) %>%
  mutate(other = ifelse(method1 == top_method, method2, method1)) %>%
  pull(other)

if (length(non_sig_from_top) > 0) {
  cat(sprintf("**Methods not significantly different from %s**: %s\n",
              top_method, paste(non_sig_from_top, collapse = ", ")))
}
```

### Recommendations

| Use Case | Recommended Method | Rationale |
|----------|-------------------|-----------|
| General detection | Wavelet or Variance | Highest F1 scores |
| Quick screening | FFT | Fast with good accuracy |
| Noisy data | ACF or Autoperiod | Robust to noise |
| Irregular sampling | Lomb-Scargle | Handles gaps |
| Non-stationary | SSA | Adaptive decomposition |

## Cleanup

```{r}
#| label: cleanup
#| cache: false

dbExecute(con, "DROP TABLE IF EXISTS benchmark_curves")
dbExecute(con, "DROP TABLE IF EXISTS challenge_curves")
dbDisconnect(con, shutdown = TRUE)
```

## Session Info

```{r}
#| label: session-info

sessionInfo()
```
